{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68852e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from functions import *\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5accdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticker_list = ['REE', 'SAM', 'HAP', 'GMD', 'GIL', 'TMS', 'SAV', 'DHA', 'MHC', 'HAS'] # 10 stocks with the most observations\n",
    "ticker_list = ['REE', 'SAM', 'HAP'] # 3 stocks with the most observations\n",
    "limits = {\n",
    "    'hose':0.07,\n",
    "    'hnx':0.1,\n",
    "    'upcom':0.15\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb0a962e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of observations removed: 1.05%\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "REE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SAM",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "HAP",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "bb513852-b756-4fd5-8e72-ec91ca957b41",
       "rows": [
        [
         "count",
         "5951.0",
         "5951.0",
         "5951.0"
        ],
        [
         "mean",
         "0.0010912796127533122",
         "0.0006992991984033923",
         "0.0007700803761068733"
        ],
        [
         "std",
         "0.021411474695274997",
         "0.023732509900508746",
         "0.024893963733018585"
        ],
        [
         "min",
         "-0.06997105493074274",
         "-0.06999876644275715",
         "-0.0699626865671642"
        ],
        [
         "25%",
         "-0.009689088524274747",
         "-0.011620065212613162",
         "-0.012434002527699717"
        ],
        [
         "50%",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "75%",
         "0.011761445323983155",
         "0.012037072645464164",
         "0.012855287857850839"
        ],
        [
         "max",
         "0.06996220262358266",
         "0.0699185460709073",
         "0.0699271963910848"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>REE</th>\n",
       "      <th>SAM</th>\n",
       "      <th>HAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5951.000000</td>\n",
       "      <td>5951.000000</td>\n",
       "      <td>5951.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.021411</td>\n",
       "      <td>0.023733</td>\n",
       "      <td>0.024894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.069971</td>\n",
       "      <td>-0.069999</td>\n",
       "      <td>-0.069963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.009689</td>\n",
       "      <td>-0.011620</td>\n",
       "      <td>-0.012434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.011761</td>\n",
       "      <td>0.012037</td>\n",
       "      <td>0.012855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.069962</td>\n",
       "      <td>0.069919</td>\n",
       "      <td>0.069927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker          REE          SAM          HAP\n",
       "count   5951.000000  5951.000000  5951.000000\n",
       "mean       0.001091     0.000699     0.000770\n",
       "std        0.021411     0.023733     0.024894\n",
       "min       -0.069971    -0.069999    -0.069963\n",
       "25%       -0.009689    -0.011620    -0.012434\n",
       "50%        0.000000     0.000000     0.000000\n",
       "75%        0.011761     0.012037     0.012855\n",
       "max        0.069962     0.069919     0.069927"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read and merge into 1 dataset\n",
    "\n",
    "if \"stock_data.csv\" in os.listdir(\"data\"):\n",
    "    merged_df = pd.read_csv(\n",
    "        os.path.join(\"data\", \"stock_data.csv\"),\n",
    "        index_col=None\n",
    "    ).assign(\n",
    "        date = lambda df : pd.to_datetime(df[\"date\"])\n",
    "    )\n",
    "else:\n",
    "    # Read and merge data\n",
    "    hnx = pd.read_csv(os.path.join(\"data\", \"CafeF.HNX.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"hnx\"\n",
    "    )\n",
    "    hsx = pd.read_csv(os.path.join(\"data\", \"CafeF.HSX.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"hose\"\n",
    "    )\n",
    "    upcom = pd.read_csv(os.path.join(\"data\", \"CafeF.UPCOM.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"upcom\"\n",
    "    )\n",
    "    indexes = pd.read_csv(os.path.join(\"data\", \"CafeF.INDEX.Upto06.08.2025.csv\")).assign(\n",
    "        floor = \"index\"\n",
    "    )\n",
    "\n",
    "    # Rename columns\n",
    "    hnx, hsx, upcom, indexes = [\n",
    "        df.rename(columns={\n",
    "            \"<Ticker>\":\"ticker\",\n",
    "            \"<DTYYYYMMDD>\":\"date\",\n",
    "            \"<Open>\":\"open\",\n",
    "            \"<High>\":\"high\",\n",
    "            \"<Low>\":\"low\",\n",
    "            \"<Close>\":\"close\",\n",
    "            \"<Volume>\":\"volume\"\n",
    "        }) for df in [hnx, hsx, upcom, indexes]\n",
    "    ]\n",
    "        \n",
    "    # Merge and clean data\n",
    "    # UPCOM has missing tickers for some reason\n",
    "    merged_df = pd.concat(\n",
    "        [hnx, hsx, upcom, indexes],\n",
    "        axis=0\n",
    "    ).reset_index(drop=True).dropna(subset=\"ticker\")\\\n",
    "    .assign(\n",
    "        date=lambda df : df[\"date\"].astype(str).apply(lambda x: datetime.strptime(x, \"%Y%m%d\").date())\n",
    "    )\n",
    "    merged_df.to_csv(\n",
    "        os.path.join(\"data\", \"stock_data.csv\"),\n",
    "        index=False\n",
    "    ) # Save merged data to save time in future runs\n",
    "\n",
    "\n",
    "# Data cleaning and merging\n",
    "\n",
    "data = merged_df[[\"date\", \"ticker\", \"floor\", \"close\"]].sort_values([\"ticker\", \"date\"]).assign(\n",
    "    returns = lambda df : df.groupby(\"ticker\")[\"close\"].pct_change(),\n",
    "    log_returns_pct = lambda df : np.log(df[\"close\"] / df.groupby(\"ticker\")[\"close\"].shift(1))*100\n",
    ")\n",
    "\n",
    "data = data.loc[data[\"ticker\"].str.len()==3] # Eliminate ETF, and indeces\n",
    "\n",
    "data[\"limit\"] = data[\"floor\"].map(limits)\n",
    "outliers = data.loc[data[\"returns\"].abs() > data[\"limit\"]]\n",
    "clean_df = data.drop(outliers.index) # Remove outliers\n",
    "print(f\"% of observations removed: {round((len(outliers)/len(data))*100, 2)}%\")\n",
    "\n",
    "# NOTE: try out different samples of stocks\n",
    "pivoted_df = clean_df.pivot_table(values=\"returns\", index=\"date\", columns=\"ticker\") # Pivot data for better usability\n",
    "pivoted_df = pivoted_df[ticker_list].dropna()\n",
    "\n",
    "display(pivoted_df.describe())\n",
    "train_df, test_df = split_train_test(pivoted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "418024bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(params, returns):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood (Gaussian QML) for a single return series. Used to find the optimize parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # omega, alpha, beta = params\n",
    "    # Try fixing omega to prevent covariance exploding\n",
    "    _, alpha, beta = params\n",
    "    omega = np.var(returns)\n",
    "\n",
    "\n",
    "    if (omega <= 0) or (alpha < 0) or (beta < 0) or (alpha + beta >= 0.9999): # Check condition omega > 0, alpha, beta > 0 and alpha + beta < 1\n",
    "        return np.inf \n",
    "    \n",
    "    n_period = returns.size\n",
    "    variances = np.empty(n_period) # Array of variance\n",
    "\n",
    "    variance_0 = np.var(returns) if np.var(returns) > 1e-12 else 1.0 # To ensure positive definiteness\n",
    "\n",
    "    variances[0] = variance_0 # Use sample variance as the first variance\n",
    "\n",
    "    for t in range(1, n_period):\n",
    "        # Diagonal matrix of variances\n",
    "        variances[t] = omega + alpha*returns[t-1]**2 + beta*variances[t-1] # Univariate GARCH\n",
    "        if not np.isfinite(variances[t]) or variances[t] <= 1e-16:\n",
    "            return np.inf # Ensure positive definiteness\n",
    "        \n",
    "    log_likelihood = -0.5 * (np.log(2*np.pi) + np.log(variances) + (returns**2)/variances)\n",
    "\n",
    "    return -np.sum(log_likelihood)\n",
    "\n",
    "\n",
    "def univariate_garch(returns: np.ndarray, x0=(1e-6, 0.05, 0.9)):\n",
    "    \"\"\"\n",
    "    Fit Univariate GARCH\n",
    "    Args:\n",
    "        returns: np.array of return\n",
    "        x0: inital parameters\n",
    "    Return:\n",
    "        DCC input parameters\n",
    "        \"omega\":omega,\n",
    "        \"alpha\":alpha,\n",
    "        \"beta\":beta,\n",
    "        \"variances\": variances,\n",
    "        \"residuals\":resid_standardized,\n",
    "        \"success\":ugarch.success\n",
    "    \"\"\"\n",
    "\n",
    "    returns = np.asanyarray(returns).astype(float)\n",
    "    returns = returns - np.mean(returns) # Demean return\n",
    "    \n",
    "    bounds = [\n",
    "        (1e-12, None), # Must be positive\n",
    "        (0.0, 1.0), # Can be semipositive\n",
    "        (0.0, 1.0) # Can be semipositive\n",
    "    ]\n",
    "    constraints = (\n",
    "        {\n",
    "            \"type\":'ineq',\n",
    "            \"fun\": lambda p: 0.999 - (p[1] + p[2]) # Ensure alpha + beta < 1\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    ugarch = minimize(\n",
    "        negative_log_likelihood, x0,\n",
    "        args=(returns,),\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "        method=\"SLSQP\"\n",
    "    )\n",
    "\n",
    "    omega, alpha, beta = ugarch.x\n",
    "\n",
    "    # Conditional variances and standardized residuals\n",
    "    n_period = returns.size\n",
    "    variances = np.empty(n_period) # Initialize diagonal matrix of variances\n",
    "    variances[0] = np.var(returns) if np.var(returns) > 1e-12 else 1.0 # Assign first variance\n",
    "    for t in range(1, n_period):    \n",
    "        # Univariate GARCH\n",
    "        variances[t] = omega + alpha*returns[t-1]**2 + beta*variances[t-1]\n",
    "    \n",
    "    resid_standardized = returns / np.sqrt(np.clip(variances, 1e-12, None)) # Standardize residuals\n",
    "\n",
    "    return {\n",
    "        \"omega\":omega,\n",
    "        \"alpha\":alpha,\n",
    "        \"beta\":beta,\n",
    "        \"variances\": variances,\n",
    "        \"residuals\":resid_standardized,\n",
    "        \"success\":ugarch.success\n",
    "    }\n",
    "\n",
    "\n",
    "def fit_univariate_garch(df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Fit GARCH(1, 1) for each stock column\n",
    "    Args:\n",
    "        df: pd.DataFrame. Should be cleaned off NA\n",
    "    \"\"\"\n",
    "\n",
    "    n_period, n_asset = df.shape # Get period length and number of asset\n",
    "    variance_mtrx = np.zeros((n_period, n_asset)) # Initalize matrix of conditional variances for each asset\n",
    "    residual_mtrx = np.zeros((n_period, n_asset)) # Initialize matrix of standardized residuals for each assset\n",
    "    params = {}\n",
    "\n",
    "    # Fit Univariate GARCH to each stock returns\n",
    "    for i, col in enumerate(df.columns): \n",
    "        ugarch = univariate_garch(df[col].values)\n",
    "        params[col] = {\n",
    "            key:ugarch[key] for key in [\"omega\", \"alpha\", \"beta\", \"success\"]\n",
    "        }\n",
    "        variance_mtrx[:, i] = ugarch[\"variances\"]\n",
    "        residual_mtrx[:, i] = ugarch[\"residuals\"]\n",
    "\n",
    "    return variance_mtrx, residual_mtrx, params # D_t^2, eps_t, params\n",
    "\n",
    "# DCC estimation\n",
    "def dcc_NLL(params, residuals):\n",
    "    '''\n",
    "    Return negative correlation log-likelihood for DCC(1,1). Residuals is a matrix of TxM\n",
    "    ''' \n",
    "    alpha, beta = params\n",
    "    if (alpha < 0) or (beta < 0) or (alpha + beta >= 0.9999): # conditions\n",
    "        return np.inf\n",
    "    \n",
    "    n_period, n_asset = residuals.shape\n",
    "    # Unconditional correlation of residuals\n",
    "    S = np.corrcoef(residuals.T) # Initialize correlation matrix between assets return residuals aka unconditional correlation matrix of the standardized residuals\n",
    "\n",
    "    # Initialize Q with S\n",
    "    Q = S.copy()\n",
    "    NLL = 0.0\n",
    "    for t in range(n_period):\n",
    "        # Update Q_t (if t>=1 use residual[t-1], if t=0, use previous Q)\n",
    "        if t > 0:\n",
    "            prev_resid = residuals[t-1:t, :].T # Matrix M x 1\n",
    "            # Correlation matrix of residuals\n",
    "            Q = (1-alpha-beta)*S + alpha*(prev_resid@prev_resid.T) + beta*Q # DCC estimator\n",
    "\n",
    "        # Get diagonal matrix of conditional standard deviation\n",
    "        D_t = np.sqrt( \n",
    "            np.clip(\n",
    "                np.diag(Q),\n",
    "                1e-12, # Ensure no division by zero\n",
    "                None\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Correlation matrix of the standardized residuals at time t\n",
    "        R_t = np.diag(1.0/D_t) @ Q @ np.diag(1.0/D_t) # R_t = D_t^{-1} * H_t * D_t^{-1} (Engel, 2002)\n",
    "\n",
    "        resid = residuals[t]\n",
    "\n",
    "        try:\n",
    "            # Solve R_t * x = e\n",
    "            sol = np.linalg.solve(R_t, resid) # solve R_t * x = resid\n",
    "            quadratic = resid @ sol\n",
    "            sign, logdet = np.linalg.slogdet(R_t) # Returns the sign and the natural log of determinant of R_t\n",
    "            if sign <= 0:\n",
    "                return np.inf\n",
    "        except np.linalg.LinAlgError:\n",
    "            return np.inf\n",
    "        \n",
    "        # Correlation loglikelihood contribution up to constant\n",
    "        NLL += 0.5 * (logdet + quadratic)\n",
    "    \n",
    "    return NLL\n",
    "\n",
    "\n",
    "def fit_dcc(residuals, x0=(0.2, 0.97-0.02)):\n",
    "    \"\"\"\n",
    "    Fit DCC(1,1) by minimizing negative log-likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    residuals = np.asarray(residuals)\n",
    "    bounds = [\n",
    "        (1e-8, 0.999999),\n",
    "        (1e-8, 0.999999)\n",
    "    ] # NOTE: optimize this part\n",
    "\n",
    "    constraints = (\n",
    "        {\n",
    "            \"type\":\"ineq\",\n",
    "            \"fun\": lambda p: 0.9999 - (p[0] + p[1])\n",
    "        },\n",
    "    )\n",
    "\n",
    "    dcc = minimize(\n",
    "        dcc_NLL,\n",
    "        (0.05, 0.9),\n",
    "        args=(residuals, ),\n",
    "        method=\"SLSQP\",\n",
    "        bounds=bounds,\n",
    "        constraints=constraints\n",
    "    )\n",
    "\n",
    "    alpha, beta = dcc.x\n",
    "    # Reconstruct Q_t and R_t paths\n",
    "    n_period, n_asset = residuals.shape\n",
    "    S = np.corrcoef(residuals.T)\n",
    "    Q = S.copy()\n",
    "    Q_list, R_list = [], []\n",
    "    for t in range(n_period):\n",
    "        if t > 0:\n",
    "            prev_resid = residuals[(t-1):t, :].T\n",
    "            Q = (1-alpha-beta)*S + alpha*(prev_resid@prev_resid.T) + beta*Q\n",
    "        diag_std = np.sqrt(\n",
    "            np.clip(\n",
    "                np.diag(Q),\n",
    "                1e-12,\n",
    "                None\n",
    "            )\n",
    "        )\n",
    "        R = np.diag(1.0/diag_std) @ Q @ np.diag(1.0/diag_std)\n",
    "        Q_list.append(Q.copy())\n",
    "        R_list.append(R.copy())\n",
    "    \n",
    "    return {\n",
    "        \"a\":alpha,\n",
    "        \"b\":beta,\n",
    "        \"Qt\":Q_list,\n",
    "        \"Rt\":R_list,\n",
    "        \"S\":S,\n",
    "        \"success\":dcc.success\n",
    "    }\n",
    "# Build covariance matrix H_t and forecast\n",
    "\n",
    "def build_covmatrix(var_matrix, R_list):\n",
    "    '''\n",
    "    Args:\n",
    "        var_matrix: T x M matrix of conditional variances from Univariate GARCH\n",
    "        R_list: List of correlation matrices from DCC\n",
    "    Return list of H_t = D_t * R_t * D_t from univariate GARCH and DCC R_t\n",
    "    '''\n",
    "    n_period, n_asset = var_matrix.shape\n",
    "    covmatrix_list = []\n",
    "    for t in range(n_period):\n",
    "        D = np.diag(\n",
    "            np.sqrt(var_matrix[t, :])\n",
    "        ) # diagonal matrix of conditional standard deviation\n",
    "        cov_matrix = D @ R_list[t] @ D # Conditional covariance matrix\n",
    "        covmatrix_list.append(cov_matrix)\n",
    "    \n",
    "    return covmatrix_list\n",
    "\n",
    "\n",
    "def forecast_dcc_multi_step(\n",
    "        h_last, r_last, garch_params,\n",
    "        eps_last, Q_last, dcc_params, S,\n",
    "        horizon=20\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Multi-step forecast of conditional covariance matrices under DCC-GARCH(1,1).\n",
    "\n",
    "    Args:\n",
    "        h_last : (M,) last conditional variances\n",
    "        r_last : (M,) last observed returns\n",
    "        garch_params : dict of {asset: {'omega','alpha','beta'}}\n",
    "        eps_last : (M,) last standardized residuals\n",
    "        Q_last : (M,M) last Q matrix from DCC recursion\n",
    "        dcc_params : dict {'a':..., 'b':...}\n",
    "        S : (M,M) unconditional correlation matrix of eps\n",
    "        horizon : int, number of steps ahead\n",
    "\n",
    "    Returns\n",
    "        H_path : list of (M,M) covariance forecasts\n",
    "        h_path : (horizon, M) variance forecasts\n",
    "        R_path : list of (M,M) correlation forecasts\n",
    "        Q_path : list of (M,M) Q matrices\n",
    "    \"\"\"\n",
    "    M = len(h_last)\n",
    "    a, b = dcc_params[\"a\"], dcc_params[\"b\"]\n",
    "\n",
    "    # ---------- Step 1: variance forecasts ----------\n",
    "    h_path = np.empty((horizon, M))\n",
    "\n",
    "    # 1-step-ahead: needs actual r_last\n",
    "    for j, (name, p) in enumerate(garch_params.items()):\n",
    "        omega, alpha, beta = p['omega'], p['alpha'], p['beta']\n",
    "        h_path[0, j] = omega + alpha * (r_last[j]**2) + beta * h_last[j]\n",
    "\n",
    "    # Multi-step expectation (replace r^2 with expected h)\n",
    "    for k in range(1, horizon):\n",
    "        for j, (name, p) in enumerate(garch_params.items()):\n",
    "            omega, alpha, beta = p['omega'], p['alpha'], p['beta']\n",
    "            phi = alpha + beta\n",
    "            h_path[k, j] = omega + phi * h_path[k-1, j]\n",
    "\n",
    "    # ---------- Step 2: correlation forecasts ----------\n",
    "    Q_path, R_path = [], []\n",
    "    \n",
    "    # 1-step-ahead\n",
    "    Q_next = (1 - a - b) * S + a * np.outer(eps_last, eps_last) + b * Q_last\n",
    "    Q_path.append(Q_next.copy())\n",
    "    dq = np.sqrt(np.clip(np.diag(Q_next), 1e-12, None))\n",
    "    R_path.append(Q_next / np.outer(dq, dq))\n",
    "\n",
    "    # 2..H: use expected recursion (E[eps eps'] ~ S)\n",
    "    for k in range(1, horizon):\n",
    "        Q_next = (1 - a - b) * S + a * S + b * Q_path[-1]   # simplifies to S + b*(Q_{k-1}-S)\n",
    "        Q_path.append(Q_next.copy())\n",
    "        dq = np.sqrt(np.clip(np.diag(Q_next), 1e-12, None))\n",
    "        R_path.append(Q_next / np.outer(dq, dq))\n",
    "\n",
    "    # ---------- Step 3: combine into H ----------\n",
    "    H_path = []\n",
    "    for k in range(horizon):\n",
    "        D_k = np.diag(np.sqrt(h_path[k, :]))\n",
    "        H_path.append(D_k @ R_path[k] @ D_k)\n",
    "\n",
    "    return H_path, h_path, R_path, Q_path\n",
    "\n",
    "def forecast_dcc_one_step(residuals, dcc_fit):\n",
    "    '''\n",
    "    One-step ahead forecast using last residual and last covariance matrix of standardized residuals (not a true correlation matrix)\n",
    "    '''\n",
    "    alpha, beta = dcc_fit[\"a\"], dcc_fit[\"b\"]\n",
    "    Q_last = dcc_fit[\"Qt\"][-1].copy()\n",
    "    S = np.corrcoef(residuals.T)\n",
    "    resid = residuals[-1][:, None] # Mx1\n",
    "    Q_forecast = (1-alpha-beta)*S + alpha*(resid@resid.T) + beta*Q_last\n",
    "    diag_std = np.sqrt(\n",
    "        np.clip(\n",
    "            np.diag(Q_forecast),\n",
    "            1e-12, \n",
    "            None\n",
    "        )\n",
    "    )\n",
    "    R_forecast = np.diag(1.0/diag_std) @ Q_forecast @ np.diag(1.0/diag_std)\n",
    "\n",
    "    return Q_forecast, R_forecast\n",
    "\n",
    "def forecast_H_one_step(h_last, garch_params, r_last):\n",
    "    '''\n",
    "    One-step ahead diagonal vol forecast\n",
    "    '''\n",
    "    n_asset = len(h_last)\n",
    "    variance_forecast = np.empty(n_asset) # initialize variance\n",
    "    for j, (name, p) in enumerate(garch_params.items()):\n",
    "        omega, alpha, beta = p[\"omega\"], p[\"alpha\"], p[\"beta\"]\n",
    "        variance_forecast[j] = omega + alpha*(r_last[j]**2) + beta*h_last[j]\n",
    "\n",
    "    return variance_forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94f345f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.astype(float)\n",
    "h_mat, eps_mat, garch_params = fit_univariate_garch(train_df) # Fit univariate GARCH for each stock\n",
    "dcc = fit_dcc(eps_mat) # Fit DCC(1,1) on standardized residuals\n",
    "\n",
    "covmatrix_list = build_covmatrix(h_mat, dcc[\"Rt\"]) # Get list of conditinoal covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76e6878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# one step forecast\n",
    "Q_forecast, R_forecast = forecast_dcc_one_step(eps_mat, dcc)\n",
    "h_forecast = forecast_H_one_step(\n",
    "    h_last=h_mat[-1], \n",
    "    garch_params=garch_params,\n",
    "    r_last=train_df.values[-1]\n",
    ")\n",
    "D_forecast = np.diag(\n",
    "    np.sqrt(h_forecast)\n",
    ")\n",
    "covmatrix_forecast = D_forecast @ R_forecast @ D_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eed76f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multistep forecast\n",
    "\n",
    "H_path, h_path, R_path, Q_path = forecast_dcc_multi_step(\n",
    "    h_last=h_mat[-1],\n",
    "    r_last=train_df.values[-1],\n",
    "    garch_params=garch_params,\n",
    "    eps_last=eps_mat[-1],\n",
    "    Q_last=dcc[\"Qt\"][-1],\n",
    "    dcc_params=dcc,\n",
    "    S=dcc[\"S\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "994192e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'REE': 0.3333333333333333,\n",
       " 'SAM': 0.3333333333333333,\n",
       " 'HAP': 0.3333333333333333}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_variance_portfolio(\n",
    "    covariance_matrix=H_path[-1],\n",
    "    data=train_df\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_bekk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
