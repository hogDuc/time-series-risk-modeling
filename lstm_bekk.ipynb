{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "04f8a854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'functions' from 'c:\\\\Users\\\\ADMIN\\\\Desktop\\\\FPTS\\\\Modeling Time Series Risks\\\\scripts\\\\functions.py'>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import RMSprop\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import functions\n",
    "from functions import *\n",
    "import importlib\n",
    "\n",
    "importlib.reload(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c996c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of observations removed: 1.05%\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "REE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SAM",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "HAP",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "43348e2c-097c-4ca1-bac2-3464a82dcc47",
       "rows": [
        [
         "count",
         "5951.0",
         "5951.0",
         "5951.0"
        ],
        [
         "mean",
         "0.0010912796127533122",
         "0.0006992991984033923",
         "0.0007700803761068733"
        ],
        [
         "std",
         "0.021411474695274997",
         "0.023732509900508746",
         "0.024893963733018585"
        ],
        [
         "min",
         "-0.06997105493074274",
         "-0.06999876644275715",
         "-0.0699626865671642"
        ],
        [
         "25%",
         "-0.009689088524274747",
         "-0.011620065212613162",
         "-0.012434002527699717"
        ],
        [
         "50%",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "75%",
         "0.011761445323983155",
         "0.012037072645464164",
         "0.012855287857850839"
        ],
        [
         "max",
         "0.06996220262358266",
         "0.0699185460709073",
         "0.0699271963910848"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>REE</th>\n",
       "      <th>SAM</th>\n",
       "      <th>HAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5951.000000</td>\n",
       "      <td>5951.000000</td>\n",
       "      <td>5951.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.021411</td>\n",
       "      <td>0.023733</td>\n",
       "      <td>0.024894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.069971</td>\n",
       "      <td>-0.069999</td>\n",
       "      <td>-0.069963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.009689</td>\n",
       "      <td>-0.011620</td>\n",
       "      <td>-0.012434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.011761</td>\n",
       "      <td>0.012037</td>\n",
       "      <td>0.012855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.069962</td>\n",
       "      <td>0.069919</td>\n",
       "      <td>0.069927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker          REE          SAM          HAP\n",
       "count   5951.000000  5951.000000  5951.000000\n",
       "mean       0.001091     0.000699     0.000770\n",
       "std        0.021411     0.023733     0.024894\n",
       "min       -0.069971    -0.069999    -0.069963\n",
       "25%       -0.009689    -0.011620    -0.012434\n",
       "50%        0.000000     0.000000     0.000000\n",
       "75%        0.011761     0.012037     0.012855\n",
       "max        0.069962     0.069919     0.069927"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ticker_list = ['REE', 'SAM', 'HAP', 'GMD', 'GIL', 'TMS', 'SAV', 'DHA', 'MHC', 'HAS'] # 10 stocks with the most observations\n",
    "ticker_list = ['REE', 'SAM', 'HAP'] # 3 stocks with the most observations\n",
    "limits = {\n",
    "    'hose':0.07,\n",
    "    'hnx':0.1,\n",
    "    'upcom':0.15\n",
    "}\n",
    "# Read and merge into 1 dataset\n",
    "\n",
    "if \"stock_data.csv\" in os.listdir(\"data\"):\n",
    "    merged_df = pd.read_csv(\n",
    "        os.path.join(\"data\", \"stock_data.csv\"),\n",
    "        index_col=None\n",
    "    ).assign(\n",
    "        date = lambda df : pd.to_datetime(df[\"date\"])\n",
    "    )\n",
    "else:\n",
    "    # Read and merge data\n",
    "    hnx = pd.read_csv(os.path.join(\"data\", \"CafeF.HNX.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"hnx\"\n",
    "    )\n",
    "    hsx = pd.read_csv(os.path.join(\"data\", \"CafeF.HSX.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"hose\"\n",
    "    )\n",
    "    upcom = pd.read_csv(os.path.join(\"data\", \"CafeF.UPCOM.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"upcom\"\n",
    "    )\n",
    "    indexes = pd.read_csv(os.path.join(\"data\", \"CafeF.INDEX.Upto06.08.2025.csv\")).assign(\n",
    "        floor = \"index\"\n",
    "    )\n",
    "\n",
    "    # Rename columns\n",
    "    hnx, hsx, upcom, indexes = [\n",
    "        df.rename(columns={\n",
    "            \"<Ticker>\":\"ticker\",\n",
    "            \"<DTYYYYMMDD>\":\"date\",\n",
    "            \"<Open>\":\"open\",\n",
    "            \"<High>\":\"high\",\n",
    "            \"<Low>\":\"low\",\n",
    "            \"<Close>\":\"close\",\n",
    "            \"<Volume>\":\"volume\"\n",
    "        }) for df in [hnx, hsx, upcom, indexes]\n",
    "    ]\n",
    "        \n",
    "    # Merge and clean data\n",
    "    # UPCOM has missing tickers for some reason\n",
    "    merged_df = pd.concat(\n",
    "        [hnx, hsx, upcom, indexes],\n",
    "        axis=0\n",
    "    ).reset_index(drop=True).dropna(subset=\"ticker\")\\\n",
    "    .assign(\n",
    "        date=lambda df : df[\"date\"].astype(str).apply(lambda x: datetime.strptime(x, \"%Y%m%d\").date())\n",
    "    )\n",
    "    merged_df.to_csv(\n",
    "        os.path.join(\"data\", \"stock_data.csv\"),\n",
    "        index=False\n",
    "    ) # Save merged data to save time in future runs\n",
    "\n",
    "\n",
    "# Data cleaning and merging\n",
    "\n",
    "data = merged_df[[\"date\", \"ticker\", \"floor\", \"close\"]].sort_values([\"ticker\", \"date\"]).assign(\n",
    "    returns = lambda df : df.groupby(\"ticker\")[\"close\"].pct_change(),\n",
    "    log_returns_pct = lambda df : np.log(df[\"close\"] / df.groupby(\"ticker\")[\"close\"].shift(1))*100\n",
    ")\n",
    "\n",
    "data = data.loc[data[\"ticker\"].str.len()==3] # Eliminate ETF, and indeces\n",
    "\n",
    "data[\"limit\"] = data[\"floor\"].map(limits)\n",
    "outliers = data.loc[data[\"returns\"].abs() > data[\"limit\"]]\n",
    "clean_df = data.drop(outliers.index) # Remove outliers\n",
    "print(f\"% of observations removed: {round((len(outliers)/len(data))*100, 2)}%\")\n",
    "\n",
    "# NOTE: try out different samples of stocks\n",
    "pivoted_df = clean_df.pivot_table(values=\"returns\", index=\"date\", columns=\"ticker\") # Pivot data for better usability\n",
    "pivoted_df = pivoted_df[ticker_list].dropna()\n",
    "\n",
    "display(pivoted_df.describe())\n",
    "train_df, test_df = split_train_test(pivoted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9201759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, device):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return torch.from_numpy(x).to(device=device, dtype=torch.float64)\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(device=device, dtype=torch.float64)\n",
    "    raise TypeError(\"Unsupported array type\")\n",
    "\n",
    "def lower_triangle_index(n_assets: int):\n",
    "    '''\n",
    "    Get index of lower triangular matrix\n",
    "    '''\n",
    "    rows, cols = torch.tril_indices(n_assets, n_assets)\n",
    "    diag_mask = rows == cols\n",
    "    off_mask = ~diag_mask\n",
    "    return rows, cols, diag_mask, off_mask\n",
    "\n",
    "def vec_to_lower_tri(vec: torch.Tensor, n_assets: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Map vector of length n_assets(n_assets+1)/2 to a lower-triangular matrix\n",
    "    Args:\n",
    "        vec: shape (..., n_assets(n_assets+1)/2)\n",
    "    Output:\n",
    "        shapeL (..., n_assets,n_assets)\n",
    "    \"\"\"\n",
    "\n",
    "    vec_length = n_assets * (n_assets + 1) // 2\n",
    "    assert vec.shape[-1] == vec_length, f\"Expected vector length {vec_length} for number of assets = {n_assets}, got {vec.shape[-1]}\"\n",
    "    rows, cols, _, _ = lower_triangle_index(n_assets)\n",
    "    out = vec.new_zeros(*vec.shape[:-1], n_assets, n_assets) # Initialize\n",
    "    out[..., rows, cols] = vec\n",
    "\n",
    "    return out\n",
    "\n",
    "class Static_C(nn.Module):\n",
    "    '''\n",
    "    Parameterization of the static lower-triangular C with positive diagonal to ensure semipositive definitenes of C'C\n",
    "    '''\n",
    "    def __init__(self, n_assets: int):\n",
    "        super().__init__()\n",
    "        self.n_assets = n_assets\n",
    "        self.off_idx = torch.tril_indices(n_assets, n_assets, offset=-1)\n",
    "        num_off = self.off_idx.shape[1]\n",
    "        \n",
    "        self.off_params = nn.Parameter(\n",
    "            torch.zeros(num_off, dtype=torch.float64)\n",
    "        )\n",
    "        self.diag_params = nn.Parameter(\n",
    "            torch.zeros(n_assets, dtype=torch.float64)\n",
    "        )\n",
    "    \n",
    "    def forward(self) -> torch.Tensor:\n",
    "        C = self.off_params.new_zeros(self.n_assets, self.n_assets)\n",
    "        # Off diagonals parameteres\n",
    "        C[self.off_idx[0], self.off_idx[1]] = self.off_params\n",
    "        # Diagonal parameteres\n",
    "        C[range(self.n_assets), range(self.n_assets)] = torch.nn.functional.softplus(self.diag_params) + 1e-6\n",
    "\n",
    "        return C\n",
    "\n",
    "class LSTM_Dynamic_C(nn.Module):\n",
    "    \"\"\"\n",
    "    Map past returns to dynamic element C_t, map LSTM output to lower triangular matrix C_t, and regularize the diagonal with a Swish function Swish(x) = x*sigmopoid(Beta*x), with learnable x.\n",
    "    The Swish function helps stability without forcing diagonals strictly positive\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_assets: int, hidden_size: Optional[int] = None, num_layers: int = 1, dropout:float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_assets = n_assets\n",
    "        self.length = n_assets * (n_assets + 1) // 2\n",
    "        self.hidden_size = hidden_size or max(8, n_assets) # At least equal to number of assets\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM that takes in r_{t-1} and outputs a vector length = self.length at each step\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_assets,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "            dtype=torch.float64\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(\n",
    "            self.hidden_size, \n",
    "            self.length, \n",
    "            dtype=torch.float64\n",
    "        )\n",
    "        self.beta = nn.Parameter(torch.tensor(1.0, dtype=torch.float64)) # Swish parameters beta, is learnable\n",
    "\n",
    "    def swish(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Swish(x) = x * sigmoid(beta * x)\n",
    "        return x * torch.sigmoid(self.beta * x)\n",
    "\n",
    "    def forward(self, returns: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            returns: TxM returns\n",
    "        Outputs:\n",
    "            C_t for t = 1,... T shape (T, M, M)\n",
    "        Convention: at step t we feed r_{t-1}, for t=0 we won't form C_0\n",
    "        \"\"\"\n",
    "\n",
    "        n_periods, n_assets = returns.shape\n",
    "        assert n_assets == self.n_assets\n",
    "\n",
    "        x = returns.unsqueeze(0) # Add new dimension at the beginning to (1, T, M) to be suitable with torch\n",
    "        h, _ = self.lstm(x) # (1, T, H)\n",
    "        z = self.out(h) # (1, T, L)\n",
    "        z = z.squeeze(0) # (T, L)\n",
    "        C_full = vec_to_lower_tri(z, self.n_assets)\n",
    "\n",
    "        # Apply Swish to only diagonal elements\n",
    "        rows, cols, diag_mask, off_mask = lower_triangle_index(self.n_assets)\n",
    "        diag_vals = C_full[..., rows[diag_mask], cols[diag_mask]] # Get all diagonal values\n",
    "        diag_vals = self.swish(diag_vals) # Apply Swish\n",
    "        C_full[..., rows[diag_mask], cols[diag_mask]] = diag_vals # Assign Swish-applied values to the lower-triangular matrix C\n",
    "\n",
    "        return C_full # C_t\n",
    "    \n",
    "class params(nn.Module):\n",
    "    \"\"\"\n",
    "    Static scalars a, b with constraints: a, b >= 0; a + b < 1\n",
    "    Use positive reparameterization via softplus -> normalize:\n",
    "        u = softplus(u0), v = softplus(v0); s = u + v + 1; a = u/s, b = v/s\n",
    "    Which makes a, b in (0,1) and a+b <1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.u0 = nn.Parameter(\n",
    "            torch.tensor(0.2, dtype=torch.float64)\n",
    "        )\n",
    "        self.v0 = nn.Parameter(\n",
    "            torch.tensor(0.7, dtype=torch.float64)\n",
    "        )\n",
    "    \n",
    "    def forward(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        u = torch.nn.functional.softplus(self.u0)\n",
    "        v = torch.nn.functional.softplus(self.v0)\n",
    "        s = u + v + 1.0\n",
    "        a = u/s\n",
    "        b = v/s\n",
    "        return a, b\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class LSTM_BEKK_config:\n",
    "    hidden_size: Optional[int] = None\n",
    "    num_layers: int = 1\n",
    "    dropout: float = 0.1\n",
    "    lr: float = 0.001\n",
    "    weight_decay: float = 0.0\n",
    "    epochs: int = 500\n",
    "    grad_clip: float = 10.0\n",
    "    val_split: float = 0.1\n",
    "    early_stopping_patience: int = 20\n",
    "    device: str = \"cpu\"\n",
    "    jitter: float = 1e-8 # for Cholesky stability\n",
    "    seed: int = 1\n",
    "\n",
    "class LSTM_BEKK(nn.Module):\n",
    "    \"\"\"\n",
    "    H_t = C*C' + C_t*C_t' + a*r_{t-1}*r_{t-1}' + b*H_{t-1}\n",
    "    with Gaussian log-likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_assets: int, config: Optional[LSTM_BEKK_config]=None):\n",
    "        super().__init__()\n",
    "        self.n_assets = n_assets\n",
    "        self.config = config or LSTM_BEKK_config()\n",
    "        torch.manual_seed(self.config.seed) # Set seed\n",
    "\n",
    "        self.C = Static_C(n_assets)\n",
    "        self.C_dynamic = LSTM_Dynamic_C(\n",
    "            n_assets=n_assets, \n",
    "            hidden_size=self.config.hidden_size,\n",
    "            num_layers=self.config.num_layers,\n",
    "            dropout=self.config.dropout\n",
    "        )\n",
    "        self.ab = params()\n",
    "\n",
    "        # Buffers created at fit-time \n",
    "        # NOTE: wtf is this ==================================================================\n",
    "        self.H0_: Optional[torch.Tensor] = None\n",
    "        self.mu_: Optional[torch.Tensor] = None\n",
    "        \n",
    "        self.to(\n",
    "            dtype=torch.float64,\n",
    "            device=self.config.device\n",
    "        )\n",
    "    \n",
    "    def forward_sequence(\n",
    "            self,\n",
    "            returns: torch.Tensor,\n",
    "            init_cov_matrix: Optional[torch.Tensor]=None\n",
    "        ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Implement the recursion for covariance matrix H_t. Compute covariance matrix H_t and per-step negative log-likelihood terms, given returns TxM\n",
    "        Output:\n",
    "            H: (T, M, M)\n",
    "            nll_terms: (T,) with 0.5 * (logdet(cov_matrix) + r_t' * cov_matrix **(-1)*r_t)\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        device = config.device\n",
    "        n_periods, n_assets = returns.shape\n",
    "        assert n_assets == self.n_assets\n",
    "\n",
    "        # Static C and static scalar a, b\n",
    "        C = self.C()\n",
    "        a, b = self.ab()\n",
    "\n",
    "        # Dynamic C_t\n",
    "        C_t_all = self.C_dynamic(returns)\n",
    "\n",
    "        # Initialize H_0\n",
    "        if init_cov_matrix is None:\n",
    "            cov = torch.cov(returns.T)\n",
    "            cov = cov + config.jitter * torch.eye(\n",
    "                n_assets,\n",
    "                dtype=torch.float64,\n",
    "                device=device\n",
    "            )\n",
    "            prev_cov_matrix = cov\n",
    "        else:\n",
    "            prev_cov_matrix = init_cov_matrix.to(device=device, dtype=torch.float64)\n",
    "        \n",
    "        cov_matrix_list = []\n",
    "        nll_terms = []\n",
    "\n",
    "        eye_assets = torch.eye(n_assets, dtype=torch.float64, device=device)\n",
    "\n",
    "        for t in range(n_periods):\n",
    "            prev_returns = returns[t-1].unsqueeze(0).T if t>0 else torch.zeros(n_assets, 1, dtype=torch.float64, device=device)\n",
    "            C_t = C_t_all[t]\n",
    "\n",
    "            C_static = C @ C.T\n",
    "            C_dynamic = C_t @ C_t.T\n",
    "            arch = a * (prev_returns @ prev_returns.T)\n",
    "            garch = b * prev_cov_matrix\n",
    "\n",
    "            cov_matrix = C_static + C_dynamic + arch + garch\n",
    "\n",
    "            # Stabilize for Cholesky\n",
    "            # NOTE wtf is this? -==========================\n",
    "            cov_matrix = cov_matrix + config.jitter * eye_assets\n",
    "\n",
    "            # Cholesky-based log-likelihood\n",
    "            L = torch.linalg.cholesky(cov_matrix)\n",
    "            logdet = 2.0 * torch.log(torch.diag(L)).sum()\n",
    "\n",
    "            # Solve H^{-1}*r via 2 triangular solves\n",
    "            returns_t = returns[t].unsqueeze(0).T\n",
    "            y = torch.cholesky_solve(returns_t, L) # Solves H*y=r\n",
    "            quad = (returns_t.T @ y).squeeze() # r' * H^{t-1} * r\n",
    "\n",
    "            nll_t = 0.5 * (logdet + quad)\n",
    "            nll_terms.append(nll_t)\n",
    "            cov_matrix_list.append(cov_matrix)\n",
    "\n",
    "            prev_cov_matrix = cov_matrix\n",
    "        \n",
    "        cov_matrices = torch.stack(cov_matrix_list, dim=0)\n",
    "        nll_terms = torch.stack(nll_terms, dim=0)\n",
    "\n",
    "        return cov_matrices, nll_terms\n",
    "    \n",
    "    def negative_loglik(self, returns: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        _, nll_terms = self.forward_sequence(returns, init_cov_matrix=self.H0_)\n",
    "        return nll_terms.sum()\n",
    "    \n",
    "    def fit(\n",
    "            self,\n",
    "            returns_df: pd.DataFrame,\n",
    "            verbose: bool = True\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Fit the model by minimizing the Gaussian negative log-likelihood\n",
    "        returns_df: TxM of demeaned returns\n",
    "        \"\"\"\n",
    "\n",
    "        config = self.config\n",
    "        device = config.device\n",
    "\n",
    "        returns_np = returns_df.to_numpy(dtype=float)\n",
    "        returns_tensor = to_tensor(returns_np, device=device)\n",
    "        n_periods = returns_tensor.shape[0]\n",
    "\n",
    "        # split train, valuate set\n",
    "        t_valuate = max(1, int(math.floor(config.val_split * n_periods)))\n",
    "        t_train = n_periods - t_valuate\n",
    "        r_train = returns_tensor[:t_train]\n",
    "        r_valuate = returns_tensor[t_train:]\n",
    "\n",
    "        # set H_0 from training sample\n",
    "        cov = torch.cov(r_train.T)\n",
    "        cov = cov + config.jitter * torch.eye(\n",
    "            self.n_assets,\n",
    "            dtype=torch.float64,\n",
    "            device=device\n",
    "        )\n",
    "        self.init_cov_matrix = cov.detach()\n",
    "\n",
    "        params = list(self.parameters())\n",
    "        opt = RMSprop(params, lr=config.lr, weight_decay=config.weight_decay)\n",
    "        best_val = float(\"inf\")\n",
    "        best_state = None\n",
    "        patience = config.early_stopping_patience\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            self.train()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            nll = self.negative_loglik(r_train)\n",
    "            nll.backward()\n",
    "            if config.grad_clip is not None and config.grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), config.grad_clip)\n",
    "            opt.step()\n",
    "\n",
    "            # Evaluate on validation\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                val_nll = self.negative_loglik(r_valuate).item()\n",
    "\n",
    "            if verbose and (epoch % 10 == 0 or epoch == config.epochs-1):\n",
    "                print(f\"[{epoch:04d}] train NLL : {nll.item():.3f} | val NLL : {val_nll:.3f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_nll + 1e-9 < best_val:\n",
    "                best_val = val_nll\n",
    "                best_state = {\n",
    "                    k: v.detach().clone() for k, v in self.state_dict().items()\n",
    "                }\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping at epoch {epoch}, best val NLL: {best_val:.3f}\")\n",
    "                    break\n",
    "        \n",
    "        # Restore best\n",
    "        if best_state is not None:\n",
    "            self.load_state_dict(best_state)\n",
    "\n",
    "        return {\"best_val_nll\":best_val}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def covariance(self, returns_df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute fitted covariance matrix for the full series\n",
    "        Args:\n",
    "            returns_df: demeaned returns\n",
    "        Output:\n",
    "            array of shape (n_periods, n_assets, n_assets)\n",
    "        \"\"\"\n",
    "        returns_np = returns_df.to_numpy(dtype=float)\n",
    "        returns_tensor = to_tensor(returns_np, self.config.device)\n",
    "        cov_matrix, _ = self.forward_sequence(returns_tensor, init_cov_matrix=self.H0_)\n",
    "        return cov_matrix.cpu().numpy()\n",
    "    \n",
    "    def get_params(self) -> Dict[str, torch.Tensor]:\n",
    "        a, b = self.ab()\n",
    "        return {\n",
    "            \"C_static\":self.C().detach().cpu(),\n",
    "            \"a\": a.detach().cpu(),\n",
    "            \"b\": b.detach().cpu(),\n",
    "            \"beta_swish\":self.C_dynamic.beta.detach().cpu()\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forecast_one_step(\n",
    "        self,\n",
    "        last_returns: np.ndarray,\n",
    "        last_cov: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        '''\n",
    "        One-step ahead forecast with r_T and H_T\n",
    "        Args: \n",
    "            last_returns: shape (n_assets, )\n",
    "            last_cov: shape (n_assets, n_assets) (H_T)\n",
    "        '''\n",
    "        device = self.config.device\n",
    "        returns = to_tensor(last_returns.reshape(1, -1), device)\n",
    "        C = self.C()\n",
    "        a, b = self.ab()\n",
    "        C_dynamic = self.C_dynamic(returns)[0]\n",
    "\n",
    "        cov_matrix_forecast = C @ C.T + C_dynamic @ C_dynamic.T + a * (returns @ returns.T) + b * to_tensor(last_cov, device)\n",
    "        cov_matrix_forecast = cov_matrix_forecast * self.config.jitter * torch.eye(self.n_assets, dtype=torch.float64, device=device)\n",
    "\n",
    "        return cov_matrix_forecast.cpu().numpy()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forecast_multi_step(\n",
    "        self,\n",
    "        last_returns: np.ndarray,\n",
    "        last_cov: np.ndarray,\n",
    "        steps: int = 20,\n",
    "        method: str = \"zero\"\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            method:\n",
    "            - \"zero\": feed zeros for future returns\n",
    "            - \"simulate\": simulate paths of future returns\n",
    "        \"\"\"\n",
    "        device = self.config.device\n",
    "        n_assets = self.n_assets\n",
    "        forecasts = []\n",
    "\n",
    "        r_curr = to_tensor(last_returns.reshape(1, -1), device)\n",
    "        H_curr = to_tensor(last_cov, device)\n",
    "\n",
    "        for step in range(steps):\n",
    "            C = self.C()\n",
    "            a, b = self.ab()\n",
    "            \n",
    "            # Dynamic C\n",
    "            C_t = self.C_dynamic(r_curr)[0]\n",
    "\n",
    "            if method == \"zero\" and step > 0:\n",
    "                arch = torch.zeros(\n",
    "                    n_assets, n_assets, dtype=torch.float64, device=device\n",
    "                )\n",
    "            else:\n",
    "                arch = a * (r_curr.T @ r_curr)\n",
    "\n",
    "            H_next = C @ C.T + C_t @ C_t.T + arch + b * H_curr\n",
    "            H_next = H_next + self.config.jitter * torch.eye(n_assets, dtype=torch.float64, device=device)\n",
    "\n",
    "            forecasts.append(H_next.cpu().numpy())\n",
    "\n",
    "            # Update for next iteration\n",
    "            H_curr = H_next\n",
    "            if method == \"zero\":\n",
    "                r_curr = torch.zeros_like(r_curr) # feed zeros\n",
    "            elif method == \"simulate\":\n",
    "                # sample one return path\n",
    "                L = torch.linalg.cholesky(H_next)\n",
    "                z = torch.randn(n_assets, 1, dtype=torch.float64, device=device)\n",
    "                r_curr (L @ z).T \n",
    "        \n",
    "        return np.stack(forecasts, axis=0)\n",
    "    \n",
    "def fit_lstm_bekk(\n",
    "        returns_df: pd.DataFrame,\n",
    "        hidden_size: Optional[int] = None,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "        lr: float = 0.001,\n",
    "        epochs: int = 500,\n",
    "        device: str = \"cpu\"\n",
    ") -> LSTM_BEKK:\n",
    "    n_assets = returns_df.shape[1]\n",
    "    config = LSTM_BEKK_config(\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        lr=lr,\n",
    "        epochs=epochs,\n",
    "        device=device\n",
    "    )\n",
    "    model = LSTM_BEKK(n_assets=n_assets, config=config)\n",
    "    model.fit(returns_df, verbose=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "624c683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_df = train_df - train_df.mean() # Demean return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2053a086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0000] train NLL : -552.885 | val NLL : -90.523\n",
      "[0010] train NLL : -1667.438 | val NLL : -192.840\n",
      "[0020] train NLL : -2128.596 | val NLL : -241.876\n",
      "[0030] train NLL : -2459.089 | val NLL : -277.733\n",
      "[0040] train NLL : -2731.069 | val NLL : -307.489\n",
      "[0050] train NLL : -2969.607 | val NLL : -333.700\n",
      "[0060] train NLL : -3186.307 | val NLL : -357.572\n",
      "[0070] train NLL : -3387.524 | val NLL : -379.775\n",
      "[0080] train NLL : -3577.077 | val NLL : -400.713\n",
      "[0090] train NLL : -3757.383 | val NLL : -420.645\n",
      "[0100] train NLL : -3930.028 | val NLL : -439.739\n",
      "[0110] train NLL : -4096.069 | val NLL : -458.110\n",
      "[0120] train NLL : -4256.362 | val NLL : -475.850\n",
      "[0130] train NLL : -4411.490 | val NLL : -493.022\n",
      "[0140] train NLL : -4562.002 | val NLL : -509.686\n",
      "[0150] train NLL : -4708.380 | val NLL : -525.897\n",
      "[0160] train NLL : -4851.068 | val NLL : -541.702\n",
      "[0170] train NLL : -4990.392 | val NLL : -557.132\n",
      "[0180] train NLL : -5127.019 | val NLL : -572.281\n",
      "[0190] train NLL : -5261.056 | val NLL : -587.139\n",
      "[0200] train NLL : -5392.930 | val NLL : -601.760\n",
      "[0210] train NLL : -5522.959 | val NLL : -616.180\n",
      "[0220] train NLL : -5651.392 | val NLL : -630.421\n",
      "[0230] train NLL : -5778.578 | val NLL : -644.536\n",
      "[0240] train NLL : -5904.666 | val NLL : -658.527\n",
      "[0250] train NLL : -6029.852 | val NLL : -672.420\n",
      "[0260] train NLL : -6154.297 | val NLL : -686.231\n",
      "[0270] train NLL : -6278.081 | val NLL : -699.969\n",
      "[0280] train NLL : -6401.472 | val NLL : -713.668\n",
      "[0290] train NLL : -6524.397 | val NLL : -727.315\n",
      "[0300] train NLL : -6646.965 | val NLL : -740.923\n",
      "[0310] train NLL : -6769.206 | val NLL : -754.492\n",
      "[0320] train NLL : -6891.233 | val NLL : -768.043\n",
      "[0330] train NLL : -7013.052 | val NLL : -781.570\n",
      "[0340] train NLL : -7134.667 | val NLL : -795.074\n",
      "[0350] train NLL : -7256.103 | val NLL : -808.558\n",
      "[0360] train NLL : -7377.416 | val NLL : -822.031\n",
      "[0370] train NLL : -7498.647 | val NLL : -835.495\n",
      "[0380] train NLL : -7619.759 | val NLL : -848.946\n",
      "[0390] train NLL : -7740.772 | val NLL : -862.385\n",
      "[0400] train NLL : -7861.725 | val NLL : -875.820\n",
      "[0410] train NLL : -7982.662 | val NLL : -889.254\n",
      "[0420] train NLL : -8103.539 | val NLL : -902.681\n",
      "[0430] train NLL : -8224.363 | val NLL : -916.101\n",
      "[0440] train NLL : -8345.170 | val NLL : -929.522\n",
      "[0450] train NLL : -8465.997 | val NLL : -942.946\n",
      "[0460] train NLL : -8586.802 | val NLL : -956.367\n",
      "[0470] train NLL : -8707.580 | val NLL : -969.785\n",
      "[0480] train NLL : -8828.366 | val NLL : -983.205\n",
      "[0490] train NLL : -8949.200 | val NLL : -996.633\n",
      "[0499] train NLL : -9057.961 | val NLL : -1008.726\n"
     ]
    }
   ],
   "source": [
    "model = fit_lstm_bekk(\n",
    "    returns_df=returns_df,\n",
    "    hidden_size=3, # Same as number of assets\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    "    lr=0.001,\n",
    "    epochs=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82792662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a, b = 0.4007113830192244, 0.2556415151059981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "params = model.get_params()\n",
    "print(f\"\"\"\n",
    "a, b = {params[\"a\"].item()}, {params['b'].item()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = model.covariance(returns_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352189b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.46974056e-09, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 4.09104797e-09, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 3.49805265e-09]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One step forecast\n",
    "last_return = returns_df.iloc[-1].to_numpy()\n",
    "last_cov_matrix = cov_matrix[-1]\n",
    "cov_matrix_1_step = model.forecast_one_step(\n",
    "    last_returns=last_return,\n",
    "    last_cov=last_cov_matrix\n",
    ")\n",
    "cov_matrix_1_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4948d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.24674678, 0.18141047, 0.12731048],\n",
       "        [0.18141047, 0.40841957, 0.09459668],\n",
       "        [0.12731048, 0.09459668, 0.34934679]],\n",
       "\n",
       "       [[0.2465384 , 0.18285989, 0.12751626],\n",
       "        [0.18285989, 0.41829586, 0.09473948],\n",
       "        [0.12751626, 0.09473948, 0.35962741]],\n",
       "\n",
       "       [[0.24648513, 0.18323042, 0.12756887],\n",
       "        [0.18323042, 0.42082065, 0.09477599],\n",
       "        [0.12756887, 0.09477599, 0.36225557]],\n",
       "\n",
       "       [[0.24647151, 0.18332514, 0.12758231],\n",
       "        [0.18332514, 0.42146609, 0.09478532],\n",
       "        [0.12758231, 0.09478532, 0.36292743]],\n",
       "\n",
       "       [[0.24646803, 0.18334936, 0.12758575],\n",
       "        [0.18334936, 0.42163109, 0.09478771],\n",
       "        [0.12758575, 0.09478771, 0.36309919]],\n",
       "\n",
       "       [[0.24646714, 0.18335555, 0.12758663],\n",
       "        [0.18335555, 0.42167327, 0.09478832],\n",
       "        [0.12758663, 0.09478832, 0.3631431 ]],\n",
       "\n",
       "       [[0.24646691, 0.18335713, 0.12758686],\n",
       "        [0.18335713, 0.42168406, 0.09478847],\n",
       "        [0.12758686, 0.09478847, 0.36315432]],\n",
       "\n",
       "       [[0.24646686, 0.18335753, 0.12758691],\n",
       "        [0.18335753, 0.42168681, 0.09478851],\n",
       "        [0.12758691, 0.09478851, 0.36315719]],\n",
       "\n",
       "       [[0.24646684, 0.18335764, 0.12758693],\n",
       "        [0.18335764, 0.42168752, 0.09478852],\n",
       "        [0.12758693, 0.09478852, 0.36315793]],\n",
       "\n",
       "       [[0.24646684, 0.18335766, 0.12758693],\n",
       "        [0.18335766, 0.4216877 , 0.09478853],\n",
       "        [0.12758693, 0.09478853, 0.36315811]],\n",
       "\n",
       "       [[0.24646684, 0.18335767, 0.12758693],\n",
       "        [0.18335767, 0.42168775, 0.09478853],\n",
       "        [0.12758693, 0.09478853, 0.36315816]],\n",
       "\n",
       "       [[0.24646684, 0.18335767, 0.12758693],\n",
       "        [0.18335767, 0.42168776, 0.09478853],\n",
       "        [0.12758693, 0.09478853, 0.36315817]],\n",
       "\n",
       "       [[0.24646684, 0.18335767, 0.12758693],\n",
       "        [0.18335767, 0.42168776, 0.09478853],\n",
       "        [0.12758693, 0.09478853, 0.36315818]],\n",
       "\n",
       "       [[0.24646684, 0.18335767, 0.12758693],\n",
       "        [0.18335767, 0.42168776, 0.09478853],\n",
       "        [0.12758693, 0.09478853, 0.36315818]],\n",
       "\n",
       "       [[0.24646684, 0.18335767, 0.12758693],\n",
       "        [0.18335767, 0.42168776, 0.09478853],\n",
       "        [0.12758693, 0.09478853, 0.36315818]],\n",
       "\n",
       "       [[0.24646684, 0.18335767, 0.12758693],\n",
       "        [0.18335767, 0.42168776, 0.09478853],\n",
       "        [0.12758693, 0.09478853, 0.36315818]],\n",
       "\n",
       "       [[0.24646684, 0.18335767, 0.12758693],\n",
       "        [0.18335767, 0.42168776, 0.09478853],\n",
       "        [0.12758693, 0.09478853, 0.36315818]],\n",
       "\n",
       "       [[0.24646684, 0.18335767, 0.12758693],\n",
       "        [0.18335767, 0.42168776, 0.09478853],\n",
       "        [0.12758693, 0.09478853, 0.36315818]],\n",
       "\n",
       "       [[0.24646684, 0.18335767, 0.12758693],\n",
       "        [0.18335767, 0.42168776, 0.09478853],\n",
       "        [0.12758693, 0.09478853, 0.36315818]],\n",
       "\n",
       "       [[0.24646684, 0.18335767, 0.12758693],\n",
       "        [0.18335767, 0.42168776, 0.09478853],\n",
       "        [0.12758693, 0.09478853, 0.36315818]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20 stepts forecast\n",
    "cov_matrix_20_steps = model.forecast_multi_step(\n",
    "    last_returns=last_return,\n",
    "    last_cov=last_cov_matrix,\n",
    "    steps=20,\n",
    "    method=\"zero\"\n",
    ")\n",
    "cov_matrix_20_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796fed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"lstm_bekk_cov_matrix.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(cov_matrix, f)\n",
    "\n",
    "# # Save model weights using torch.save\n",
    "# torch.save(model.state_dict(), \"lstm_bekk_model_state_dict.pt\")\n",
    "\n",
    "# Load model weights from file\n",
    "model.load_state_dict(torch.load(\"lstm_bekk_model_state_dict.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MVP\n",
    "\n",
    "horizon = 20\n",
    "\n",
    "last_return = returns_df.iloc[-1].to_numpy()\n",
    "last_cov_matrix = cov_matrix[-1]\n",
    "\n",
    "train_mean = train_df.mean()\n",
    "demean_test_df = test_df - train_mean # Demean with in-sample mean\n",
    "dates_test = demean_test_df.index\n",
    "\n",
    "port_returns, port_vars, act_covs = [], [], [] # Containers\n",
    "\n",
    "# Loop over test period in 20-days non-overlapping horizons\n",
    "for start in range(0, len(demean_test_df) - horizon + 1, horizon):\n",
    "    train_data = pd.concat([train_df, demean_test_df.iloc[:start]])\n",
    "    \n",
    "    # Forecast x step ahead\n",
    "    cov_list = model.forecast_multi_step(\n",
    "        last_returns=last_return,\n",
    "        last_cov=last_cov_matrix,\n",
    "        steps=horizon,\n",
    "        method=\"zero\"\n",
    "    )\n",
    "\n",
    "    # Aggregate to 20-days covariance forecast\n",
    "    agg_covariance = sum(cov_list)\n",
    "\n",
    "    # Get MVP weights\n",
    "    mvp_weights, weights_dict = functions.minimum_variance_portfolio(agg_covariance , train_df)\n",
    "\n",
    "    # Realized returns from next 20-days\n",
    "    horizon_return = test_df[start:start+horizon]\n",
    "\n",
    "\n",
    "    # Cummulative return\n",
    "    port_return = np.array(horizon_return) @ mvp_weights\n",
    "    port_returns.append(port_return.sum())\n",
    "\n",
    "    # Actual covariance\n",
    "    act_covariance =  horizon_return.T @ horizon_return\n",
    "    act_var = mvp_weights.T @ act_covariance @ mvp_weights\n",
    "    port_vars.append(act_var)\n",
    "    act_covs.append(act_covariance)\n",
    "\n",
    "    # Adjust for next iteration\n",
    "    last_return = np.array(horizon_return.iloc[[-1]])[0]\n",
    "    last_cov_matrix = np.array(act_covs[-1])\n",
    "\n",
    "results =  pd.DataFrame({\n",
    "    \"date\":dates_test[horizon-1::horizon][:len(port_returns)], # End of each horizon\n",
    "    \"realized_return\":port_returns,\n",
    "    \"realized_variance\":port_vars \n",
    "})\n",
    "\n",
    "print(f\"\"\"\n",
    "- Sharpe Ratio = {results[\"realized_return\"].mean()/results[\"realized_return\"].std()} \\n\n",
    "- Frobenius norm = {np.linalg.norm(agg_covariance - act_covariance, \"fro\")**2}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_bekk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
