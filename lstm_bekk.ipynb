{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f8a854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'functions' from 'c:\\\\Users\\\\ADMIN\\\\Desktop\\\\FPTS\\\\Modeling Time Series Risks\\\\scripts\\\\functions.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import RMSprop\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import functions\n",
    "from functions import *\n",
    "import importlib\n",
    "\n",
    "importlib.reload(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c996c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of observations removed: 1.05%\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "REE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SAM",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "HAP",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "1a75b09a-1da3-41bf-8a58-d95bd640855d",
       "rows": [
        [
         "count",
         "5951.0",
         "5951.0",
         "5951.0"
        ],
        [
         "mean",
         "0.0010912796127533122",
         "0.0006992991984033923",
         "0.0007700803761068733"
        ],
        [
         "std",
         "0.021411474695274997",
         "0.023732509900508746",
         "0.024893963733018585"
        ],
        [
         "min",
         "-0.06997105493074274",
         "-0.06999876644275715",
         "-0.0699626865671642"
        ],
        [
         "25%",
         "-0.009689088524274747",
         "-0.011620065212613162",
         "-0.012434002527699717"
        ],
        [
         "50%",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "75%",
         "0.011761445323983155",
         "0.012037072645464164",
         "0.012855287857850839"
        ],
        [
         "max",
         "0.06996220262358266",
         "0.0699185460709073",
         "0.0699271963910848"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>REE</th>\n",
       "      <th>SAM</th>\n",
       "      <th>HAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5951.000000</td>\n",
       "      <td>5951.000000</td>\n",
       "      <td>5951.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.021411</td>\n",
       "      <td>0.023733</td>\n",
       "      <td>0.024894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.069971</td>\n",
       "      <td>-0.069999</td>\n",
       "      <td>-0.069963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.009689</td>\n",
       "      <td>-0.011620</td>\n",
       "      <td>-0.012434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.011761</td>\n",
       "      <td>0.012037</td>\n",
       "      <td>0.012855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.069962</td>\n",
       "      <td>0.069919</td>\n",
       "      <td>0.069927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker          REE          SAM          HAP\n",
       "count   5951.000000  5951.000000  5951.000000\n",
       "mean       0.001091     0.000699     0.000770\n",
       "std        0.021411     0.023733     0.024894\n",
       "min       -0.069971    -0.069999    -0.069963\n",
       "25%       -0.009689    -0.011620    -0.012434\n",
       "50%        0.000000     0.000000     0.000000\n",
       "75%        0.011761     0.012037     0.012855\n",
       "max        0.069962     0.069919     0.069927"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ticker_list = ['REE', 'SAM', 'HAP', 'GMD', 'GIL', 'TMS', 'SAV', 'DHA', 'MHC', 'HAS'] # 10 stocks with the most observations\n",
    "ticker_list = ['REE', 'SAM', 'HAP'] # 3 stocks with the most observations\n",
    "limits = {\n",
    "    'hose':0.07,\n",
    "    'hnx':0.1,\n",
    "    'upcom':0.15\n",
    "}\n",
    "# Read and merge into 1 dataset\n",
    "\n",
    "if \"stock_data.csv\" in os.listdir(\"data\"):\n",
    "    merged_df = pd.read_csv(\n",
    "        os.path.join(\"data\", \"stock_data.csv\"),\n",
    "        index_col=None\n",
    "    ).assign(\n",
    "        date = lambda df : pd.to_datetime(df[\"date\"])\n",
    "    )\n",
    "else:\n",
    "    # Read and merge data\n",
    "    hnx = pd.read_csv(os.path.join(\"data\", \"CafeF.HNX.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"hnx\"\n",
    "    )\n",
    "    hsx = pd.read_csv(os.path.join(\"data\", \"CafeF.HSX.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"hose\"\n",
    "    )\n",
    "    upcom = pd.read_csv(os.path.join(\"data\", \"CafeF.UPCOM.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"upcom\"\n",
    "    )\n",
    "    indexes = pd.read_csv(os.path.join(\"data\", \"CafeF.INDEX.Upto06.08.2025.csv\")).assign(\n",
    "        floor = \"index\"\n",
    "    )\n",
    "\n",
    "    # Rename columns\n",
    "    hnx, hsx, upcom, indexes = [\n",
    "        df.rename(columns={\n",
    "            \"<Ticker>\":\"ticker\",\n",
    "            \"<DTYYYYMMDD>\":\"date\",\n",
    "            \"<Open>\":\"open\",\n",
    "            \"<High>\":\"high\",\n",
    "            \"<Low>\":\"low\",\n",
    "            \"<Close>\":\"close\",\n",
    "            \"<Volume>\":\"volume\"\n",
    "        }) for df in [hnx, hsx, upcom, indexes]\n",
    "    ]\n",
    "        \n",
    "    # Merge and clean data\n",
    "    # UPCOM has missing tickers for some reason\n",
    "    merged_df = pd.concat(\n",
    "        [hnx, hsx, upcom, indexes],\n",
    "        axis=0\n",
    "    ).reset_index(drop=True).dropna(subset=\"ticker\")\\\n",
    "    .assign(\n",
    "        date=lambda df : df[\"date\"].astype(str).apply(lambda x: datetime.strptime(x, \"%Y%m%d\").date())\n",
    "    )\n",
    "    merged_df.to_csv(\n",
    "        os.path.join(\"data\", \"stock_data.csv\"),\n",
    "        index=False\n",
    "    ) # Save merged data to save time in future runs\n",
    "\n",
    "\n",
    "# Data cleaning and merging\n",
    "\n",
    "data = merged_df[[\"date\", \"ticker\", \"floor\", \"close\"]].sort_values([\"ticker\", \"date\"]).assign(\n",
    "    returns = lambda df : df.groupby(\"ticker\")[\"close\"].pct_change(),\n",
    "    log_returns_pct = lambda df : np.log(df[\"close\"] / df.groupby(\"ticker\")[\"close\"].shift(1))*100\n",
    ")\n",
    "\n",
    "data = data.loc[data[\"ticker\"].str.len()==3] # Eliminate ETF, and indeces\n",
    "\n",
    "data[\"limit\"] = data[\"floor\"].map(limits)\n",
    "outliers = data.loc[data[\"returns\"].abs() > data[\"limit\"]]\n",
    "clean_df = data.drop(outliers.index) # Remove outliers\n",
    "print(f\"% of observations removed: {round((len(outliers)/len(data))*100, 2)}%\")\n",
    "\n",
    "# NOTE: try out different samples of stocks\n",
    "pivoted_df = clean_df.pivot_table(values=\"returns\", index=\"date\", columns=\"ticker\") # Pivot data for better usability\n",
    "pivoted_df = pivoted_df[ticker_list].dropna()\n",
    "\n",
    "display(pivoted_df.describe())\n",
    "train_df, test_df = split_train_test(pivoted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38648814",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = train_df.mean()\n",
    "dm_train_df, dm_test_df = train_df - mean_train, test_df - mean_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "624c683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1b08b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM netwrodk to generate dynamic Ct\n",
    "class LSTM_Components(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers:int):\n",
    "        \"\"\"\n",
    "        Initialize LSTM network\n",
    "        Args: \n",
    "            input_size: number of assets\n",
    "            hidden_size: number of features in the hidden state\n",
    "            num_layers: number of recurrent layers\n",
    "        \"\"\"\n",
    "        super(LSTM_Components, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, batch_first=True\n",
    "        )\n",
    "\n",
    "        # The output size is n*(n+1)/2, which is the number of elements in the lower triangular matrix\n",
    "\n",
    "        num_output_elememts = int(input_size * (input_size + 1) / 2)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, num_output_elememts)\n",
    "        # Learnable parameter for the Swish activation function\n",
    "        self.beta = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, h_prev: torch.Tensor, c_prev: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of LSTM component\n",
    "        Args:\n",
    "            x: Input tensor of asset returns (demeaned)\n",
    "            h_prev: Previous hidden state\n",
    "            c_prev: previous cell state\n",
    "        Returns:\n",
    "            C_t: Lower triangular matrix C_t\n",
    "            h_new: New hidden state\n",
    "            c_new: New cell state\n",
    "        \"\"\"\n",
    "        # Ensure input is 3D for LSTM, shape (batch_size, seq_length, input_size)\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        output, (h_new, c_new) = self.lstm(x, (h_prev, c_prev))\n",
    "\n",
    "        # Pass LSTM output through linear layer\n",
    "        c_t_flat = self.linear(output.squeeze(1))\n",
    "\n",
    "        # Reshape flat vector to a lower triangular matrix C_t\n",
    "        C_t = torch.zeros(\n",
    "            x.shape[2],\n",
    "            x.shape[2],\n",
    "            device=x.device\n",
    "        ) # Initialize matrix n_assets x n_assets\n",
    "        tril_indices = torch.tril_indices(\n",
    "            row=x.shape[2],\n",
    "            col=x.shape[2],\n",
    "            offset=0\n",
    "        ) # Get lower-triangular indices\n",
    "        C_t[tril_indices[0], tril_indices[1]] = c_t_flat.squeeze(0)\n",
    "\n",
    "        # Apply Swish activation to diagonal elements for regularization\n",
    "        diag_indices = range(x.shape[2])\n",
    "        diag_elements = C_t.diag()\n",
    "        swish_activation = diag_elements * torch.sigmoid(self.beta * diag_elements)\n",
    "        C_t[diag_indices, diag_indices] = swish_activation\n",
    "\n",
    "        return C_t, h_new, c_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b730b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_BEKK(nn.Module):\n",
    "    \"\"\"\n",
    "    defines the full LSTM-BEKK model\n",
    "    \"\"\"\n",
    "    def __init__(self, num_assets:int, hidden_size:int, num_layers:int):\n",
    "        \"\"\"\n",
    "        Initialize LSTM-BEKK model\n",
    "        Args: \n",
    "            num_assets: Number of assets in portfolio\n",
    "            hidden_size: The hidden size for LSTM component\n",
    "            num_layers: Number of layers for LSTM component\n",
    "        \"\"\"\n",
    "        super(LSTM_BEKK, self).__init__()\n",
    "        self.num_assets = num_assets\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initialize LSTM component\n",
    "        self.lstm_component = LSTM_Components(num_assets, hidden_size, num_layers)\n",
    "\n",
    "        # Initialize BEKK parameters\n",
    "        # Static C (lower triangular)\n",
    "        c_init = torch.randn(self.num_assets, self.num_assets)\n",
    "        self.C = nn.Parameter(torch.tril(c_init))\n",
    "\n",
    "        # Scalar parameters a and b\n",
    "        self.a = nn.Parameter(torch.rand(1))\n",
    "        self.b = nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, returns: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Defines the forward pass to compute the sequence of covariance matrices\n",
    "        Args:\n",
    "            returns: Tensor of demeaned returns n_periods x n_assets\n",
    "        Returns:\n",
    "            Tensor of conditional covariance matrices of shape (n_periods, n_assets, n_assets)\n",
    "            The final hidden state\n",
    "            The final cell state\n",
    "        \"\"\"\n",
    "        n_periods, n_assets = returns.shape\n",
    "        covariance_list = [] # Container\n",
    "\n",
    "        # Initialize hidden states and cell states\n",
    "        hidden_t = torch.zeros(\n",
    "            self.num_layers, 1, self.hidden_size, device=returns.device\n",
    "        )\n",
    "        cell_t = torch.zeros(\n",
    "            self.num_layers, 1, self.hidden_size, device=returns.device\n",
    "        )\n",
    "\n",
    "        # Initialize H_0 as the unconditional covariance of returns\n",
    "        cov_matrix = torch.cov(returns.T)\n",
    "\n",
    "        # Static covariance component C'C\n",
    "        static_C = self.C @ self.C.T\n",
    "\n",
    "        for t in range(n_periods):\n",
    "            if t > 0:\n",
    "                prev_returns = returns[t-1, :].unsqueeze(0)\n",
    "            else:\n",
    "                prev_returns = torch.zeros_like(returns[0, :]).unsqueeze(0)\n",
    "            \n",
    "            # Get dynamic component C_t from LSTM\n",
    "            C_t, hidden_t, cell_t = self.lstm_component(prev_returns, hidden_t, cell_t)\n",
    "            dynamic_C = C_t @ C_t.T\n",
    "\n",
    "            # Previous shock component\n",
    "            prev_shock = prev_returns @ prev_returns.T\n",
    "\n",
    "            # Apply constraints: a, b >=0 and a + b < 1\n",
    "            a_constrained = torch.clamp(self.a, min=0)\n",
    "            b_constrained = torch.clamp(self.b, min=0)\n",
    "            if a_constrained + b_constrained >= 1.0:\n",
    "                total = a_constrained + b_constrained\n",
    "                a_constrained = a_constrained / (total+1e-6)\n",
    "                b_constrained = b_constrained / (total+1e-6)\n",
    "\n",
    "            # Calculate covariance matrix using LSTM-BEKK\n",
    "            cov_matrix = static_C + dynamic_C + a_constrained * prev_shock + b_constrained * cov_matrix\n",
    "            \n",
    "            # Ensure the covariance matrix is positive semi-definite NOTE:????\n",
    "            cov_matrix = (cov_matrix + cov_matrix.T) / 2 \n",
    "\n",
    "            covariance_list.append(cov_matrix)\n",
    "        \n",
    "        return torch.stack(covariance_list), hidden_t, cell_t\n",
    "    \n",
    "def negative_log_likelihood(returns: torch.Tensor, covariance_list: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Calculates the negative log-likelihood for LSTM-BEKK model\n",
    "    Args: \n",
    "        returns: The tensor of demeaned returns\n",
    "        covariance_list: List of conditional covariance matrices\n",
    "    Returns:\n",
    "        Total negative log-likelihood\n",
    "    \"\"\"\n",
    "    n_periods, n_assets = returns.shape\n",
    "    log_likelihood = 0.0\n",
    "    for t in range(n_periods):\n",
    "        cov_matrix = covariance_list[t]\n",
    "        returns_t = returns[t, :].unsqueeze(1)\n",
    "\n",
    "        # Add a small value to the diagnoal for stability\n",
    "        cov_matrix = cov_matrix + torch.eye(n_assets, device=cov_matrix.device) * 1e-6\n",
    "\n",
    "        logdet_H = torch.logdet(cov_matrix)\n",
    "        term_1 = n_assets * np.log(2 * np.pi)\n",
    "        term_2 = logdet_H\n",
    "        term_3 = returns_t.T @ torch.inverse(cov_matrix) @ returns_t\n",
    "        log_likelihood += term_1 + term_2 + term_3\n",
    "\n",
    "    return 0.5 * log_likelihood\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_data: torch.Tensor,\n",
    "    epochs:int =300,\n",
    "    learning_rate: float =0.001\n",
    "):\n",
    "    \"\"\"\n",
    "    Train LSTM-BEKK Model\n",
    "    Args:\n",
    "        model: LSTM-BEKK model instance\n",
    "        train_data: Training data deameaned returns\n",
    "        epochs: Number of training epochs\n",
    "        learning_rate: learning rate for the optimizer\n",
    "    \"\"\"\n",
    "    # Use RMSprop optimizer\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get covariance matrices\n",
    "        covariance_list, _, _ = model(train_data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = negative_log_likelihood(train_data, covariance_list)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "def forecast(\n",
    "    model: nn.Module,\n",
    "    last_return: torch.Tensor,\n",
    "    last_cov: torch.Tensor,\n",
    "    final_h: torch.Tensor,\n",
    "    final_c: torch.Tensor,\n",
    "    n_steps: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates n-steps ahead forecasts for the covariance matrix\n",
    "    Args:\n",
    "        model: Trained LSTM-BEKK Model\n",
    "        last_return: Last observed return vector\n",
    "        last_cov: Last estimated covariance matrix\n",
    "        final_h: The final hidden state from training\n",
    "        final_c: The final cell state from training \n",
    "        n_step: Number of steps to forecast ahead\n",
    "    Returns:\n",
    "        A list of forecasted covariance matrices\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    cov_matrix_fc = []\n",
    "\n",
    "    # Initialize hidden states and cell states from last training step\n",
    "    hidden_t = final_h\n",
    "    cell_t = final_c\n",
    "    current_r = last_return\n",
    "    current_cov_matrix = last_cov\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(n_steps):\n",
    "            # Get dynamic component\n",
    "            C_t, hidden_t, cell_t = model.lstm_component(\n",
    "                current_r.unsqueeze(0), hidden_t, cell_t\n",
    "            )\n",
    "            dynamic_C = C_t @ C_t.T\n",
    "\n",
    "            # Shock component\n",
    "            shock = current_r @ current_r.T\n",
    "\n",
    "            # Static C\n",
    "            static_C = model.C @ model.C.T\n",
    "\n",
    "            # Constraints\n",
    "            a_constrained = torch.clamp(model.a, min=0)\n",
    "            b_constrained = torch.clamp(model.b, min=0)\n",
    "            if a_constrained + b_constrained >= 1.0:\n",
    "                total = a_constrained + b_constrained\n",
    "                a_constrained = a_constrained / (total + 1e-6)\n",
    "                b_constrained = b_constrained / (total + 1e-6)\n",
    "\n",
    "            # Forecast next covariance matrix\n",
    "            next_cov_matrix = static_C + dynamic_C + a_constrained * shock + b_constrained * current_cov_matrix\n",
    "            cov_matrix_fc.append(next_cov_matrix)\n",
    "\n",
    "            # Update for next iteration\n",
    "            current_cov_matrix = next_cov_matrix\n",
    "            current_r = torch.zeros_like(last_return) # Assume expected return = 0 for forecast\n",
    "        \n",
    "        return cov_matrix_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba17107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Epoch [10/300], Loss: 8180.7524\n",
      "Epoch [20/300], Loss: 5933.4380\n",
      "Epoch [30/300], Loss: 4821.1152\n",
      "Epoch [40/300], Loss: 3682.7163\n",
      "Epoch [50/300], Loss: 2537.7466\n",
      "Epoch [60/300], Loss: 1332.3590\n",
      "Epoch [70/300], Loss: -10.0705\n",
      "Epoch [80/300], Loss: -1290.0580\n",
      "Epoch [90/300], Loss: -1821.0259\n",
      "Epoch [100/300], Loss: -2730.6948\n",
      "Epoch [110/300], Loss: -3716.8037\n",
      "Epoch [120/300], Loss: -4979.0229\n",
      "Epoch [130/300], Loss: -6477.3521\n",
      "Epoch [140/300], Loss: -8579.3262\n",
      "Epoch [150/300], Loss: -11277.4453\n",
      "Epoch [160/300], Loss: -12541.2178\n",
      "Epoch [170/300], Loss: -12569.1016\n",
      "Epoch [180/300], Loss: -12916.4258\n",
      "Epoch [190/300], Loss: -13103.9482\n",
      "Epoch [200/300], Loss: -13158.0332\n",
      "Epoch [210/300], Loss: -13240.0361\n",
      "Epoch [220/300], Loss: -13305.3447\n",
      "Epoch [230/300], Loss: -13363.3564\n",
      "Epoch [240/300], Loss: -13424.2734\n",
      "Epoch [250/300], Loss: -13485.3867\n",
      "Epoch [260/300], Loss: -13578.0225\n",
      "Epoch [270/300], Loss: -13578.0547\n",
      "Epoch [280/300], Loss: -13740.8271\n",
      "Epoch [290/300], Loss: -13802.6631\n",
      "Epoch [300/300], Loss: -13872.1357\n",
      "Training finished\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forecast() got an unexpected keyword argument 'last_returns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m last_returns \u001b[38;5;241m=\u001b[39m train_tensor[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m     33\u001b[0m last_cov \u001b[38;5;241m=\u001b[39m in_sample_cov_matrices[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 35\u001b[0m cov_list_tensor_fc \u001b[38;5;241m=\u001b[39m \u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlstm_bekk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_returns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_cov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_cov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_c\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[0;32m     39\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m cov_list_fc \u001b[38;5;241m=\u001b[39m [H\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m H \u001b[38;5;129;01min\u001b[39;00m cov_list_tensor_fc]\n",
      "\u001b[1;31mTypeError\u001b[0m: forecast() got an unexpected keyword argument 'last_returns'"
     ]
    }
   ],
   "source": [
    "# Convert train data to torch.Tensor\n",
    "train_tensor = torch.tensor(dm_train_df.values, dtype=torch.float32)\n",
    "\n",
    "# Initialize model\n",
    "n_assets = dm_train_df.shape[1]\n",
    "lstm_bekk = LSTM_BEKK(\n",
    "    num_assets=n_assets,\n",
    "    hidden_size=32,\n",
    "    num_layers=1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Starting model training...\")\n",
    "train_model(\n",
    "    model=lstm_bekk,\n",
    "    train_data=train_tensor,\n",
    "    epochs=300,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "print(\"Training finished\")\n",
    "\n",
    "# Get in-sample covariance matrices\n",
    "lstm_bekk.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    in_sample_cov_matrices, hidden_final, cell_final = lstm_bekk(train_tensor)\n",
    "\n",
    "# Convert to a list of numpy arrays for easier inspection\n",
    "in_sample_cov_list = [H.detach().numpy() for H in in_sample_cov_matrices]\n",
    "\n",
    "# Forecast 20 steps ahead\n",
    "last_returns = train_tensor[-1, :]\n",
    "last_cov = in_sample_cov_matrices[-1]\n",
    "\n",
    "\n",
    "cov_list_tensor_fc = forecast(\n",
    "    lstm_bekk, last_return=last_returns, last_cov=last_cov, final_h=hidden_final,\n",
    "    final_c=cell_final,\n",
    "    n_steps=20\n",
    ")\n",
    "\n",
    "cov_list_fc = [H.detach().numpy() for H in cov_list_tensor_fc]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_bekk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
