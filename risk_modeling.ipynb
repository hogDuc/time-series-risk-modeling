{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c3b03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import importlib\n",
    "\n",
    "from MODELS import LSTM_BEKK_MODEL, BEKK_GARCH_MODEL, DCC_GARCH_MODEL\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d3b6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobenius_loss(H_true: np.ndarray, H_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Frobenius norm loss: ||H_true - H_pred||_F^2\n",
    "    \"\"\"\n",
    "    diff = H_true - H_pred\n",
    "    return np.linalg.norm(diff, ord=\"fro\")**2\n",
    "\n",
    "\n",
    "def stein_loss(H_true: np.ndarray, H_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Stein loss (a likelihood-based measure):\n",
    "    tr(H_true^{-1} H_pred) - log det(H_true^{-1} H_pred) - n\n",
    "    \"\"\"\n",
    "    n = H_true.shape[0]\n",
    "    try:\n",
    "        inv_H = np.linalg.inv(H_true)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # add jitter for numerical stability\n",
    "        inv_H = np.linalg.inv(H_true + 1e-8 * np.eye(n))\n",
    "\n",
    "    A = inv_H @ H_pred\n",
    "    loss = np.trace(A) - np.log(np.linalg.det(A)) - n\n",
    "    return np.real(loss)\n",
    "\n",
    "\n",
    "def correlation_loss(H_true: np.ndarray, H_pred: np.ndarray, fisher_z: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Frobenius loss between correlation matrices.\n",
    "    Optionally applies Fisher-z transform for scale adjustment.\n",
    "    \"\"\"\n",
    "    # convert to correlation matrices\n",
    "    D_true = np.sqrt(np.diag(H_true))\n",
    "    D_pred = np.sqrt(np.diag(H_pred))\n",
    "    \n",
    "    R_true = H_true / np.outer(D_true, D_true)\n",
    "    R_pred = H_pred / np.outer(D_pred, D_pred)\n",
    "\n",
    "    if fisher_z:\n",
    "        # Fisher z-transform: z = 0.5 * ln((1+r)/(1-r))\n",
    "        R_true = np.arctanh(np.clip(R_true, -0.999999, 0.999999))\n",
    "        R_pred = np.arctanh(np.clip(R_pred, -0.999999, 0.999999))\n",
    "\n",
    "    diff = R_true - R_pred\n",
    "    return np.linalg.norm(diff, ord=\"fro\")**2\n",
    "\n",
    "def portfolio_aligned_loss(H_pred, H_true, weights):\n",
    "    return ((weights.T @ H_pred @ weights) - (weights.T @ H_true @ weights))**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a8906c",
   "metadata": {},
   "source": [
    "Data cleaning:\n",
    "\n",
    "Some close price is inappropriate, which lead to over the price fluctuation limit set by the exchange, fixed this by removing the observations that have day-to-day change over the exchange limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3519574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticker_list = ['REE', 'SAM', 'HAP', 'GMD', 'GIL', 'TMS', 'SAV', 'DHA', 'MHC', 'HAS'] # 10 stocks with the most observations\n",
    "ticker_list = ['REE', 'SAM', 'HAP'] # 3 stocks with the most observations\n",
    "limits = {\n",
    "    'hose':0.07,\n",
    "    'hnx':0.1,\n",
    "    'upcom':0.15\n",
    "}\n",
    "\n",
    "# Holding period\n",
    "horizon = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f17b4245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of observations removed: 1.05%\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "REE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SAM",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "HAP",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e595e79e-cc2e-45e7-9d38-fc69eec781f8",
       "rows": [
        [
         "count",
         "5951.0",
         "5951.0",
         "5951.0"
        ],
        [
         "mean",
         "0.0010912796127533122",
         "0.0006992991984033923",
         "0.0007700803761068733"
        ],
        [
         "std",
         "0.021411474695274997",
         "0.023732509900508746",
         "0.024893963733018585"
        ],
        [
         "min",
         "-0.06997105493074274",
         "-0.06999876644275715",
         "-0.0699626865671642"
        ],
        [
         "25%",
         "-0.009689088524274747",
         "-0.011620065212613162",
         "-0.012434002527699717"
        ],
        [
         "50%",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "75%",
         "0.011761445323983155",
         "0.012037072645464164",
         "0.012855287857850839"
        ],
        [
         "max",
         "0.06996220262358266",
         "0.0699185460709073",
         "0.0699271963910848"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>REE</th>\n",
       "      <th>SAM</th>\n",
       "      <th>HAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5951.000000</td>\n",
       "      <td>5951.000000</td>\n",
       "      <td>5951.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.021411</td>\n",
       "      <td>0.023733</td>\n",
       "      <td>0.024894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.069971</td>\n",
       "      <td>-0.069999</td>\n",
       "      <td>-0.069963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.009689</td>\n",
       "      <td>-0.011620</td>\n",
       "      <td>-0.012434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.011761</td>\n",
       "      <td>0.012037</td>\n",
       "      <td>0.012855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.069962</td>\n",
       "      <td>0.069919</td>\n",
       "      <td>0.069927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker          REE          SAM          HAP\n",
       "count   5951.000000  5951.000000  5951.000000\n",
       "mean       0.001091     0.000699     0.000770\n",
       "std        0.021411     0.023733     0.024894\n",
       "min       -0.069971    -0.069999    -0.069963\n",
       "25%       -0.009689    -0.011620    -0.012434\n",
       "50%        0.000000     0.000000     0.000000\n",
       "75%        0.011761     0.012037     0.012855\n",
       "max        0.069962     0.069919     0.069927"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read and merge into 1 dataset\n",
    "\n",
    "if \"stock_data.csv\" in os.listdir(\"data\"):\n",
    "    merged_df = pd.read_csv(\n",
    "        os.path.join(\"data\", \"stock_data.csv\"),\n",
    "        index_col=None\n",
    "    ).assign(\n",
    "        date = lambda df : pd.to_datetime(df[\"date\"])\n",
    "    )\n",
    "else:\n",
    "    # Read and merge data\n",
    "    hnx = pd.read_csv(os.path.join(\"data\", \"CafeF.HNX.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"hnx\"\n",
    "    )\n",
    "    hsx = pd.read_csv(os.path.join(\"data\", \"CafeF.HSX.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"hose\"\n",
    "    )\n",
    "    upcom = pd.read_csv(os.path.join(\"data\", \"CafeF.UPCOM.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"upcom\"\n",
    "    )\n",
    "    indexes = pd.read_csv(os.path.join(\"data\", \"CafeF.INDEX.Upto06.08.2025.csv\")).assign(\n",
    "        floor = \"index\"\n",
    "    )\n",
    "\n",
    "    # Rename columns\n",
    "    hnx, hsx, upcom, indexes = [\n",
    "        df.rename(columns={\n",
    "            \"<Ticker>\":\"ticker\",\n",
    "            \"<DTYYYYMMDD>\":\"date\",\n",
    "            \"<Open>\":\"open\",\n",
    "            \"<High>\":\"high\",\n",
    "            \"<Low>\":\"low\",\n",
    "            \"<Close>\":\"close\",\n",
    "            \"<Volume>\":\"volume\"\n",
    "        }) for df in [hnx, hsx, upcom, indexes]\n",
    "    ]\n",
    "        \n",
    "    # Merge and clean data\n",
    "    # UPCOM has missing tickers for some reason\n",
    "    merged_df = pd.concat(\n",
    "        [hnx, hsx, upcom, indexes],\n",
    "        axis=0\n",
    "    ).reset_index(drop=True).dropna(subset=\"ticker\")\\\n",
    "    .assign(\n",
    "        date=lambda df : df[\"date\"].astype(str).apply(lambda x: datetime.strptime(x, \"%Y%m%d\").date())\n",
    "    )\n",
    "    merged_df.to_csv(\n",
    "        os.path.join(\"data\", \"stock_data.csv\"),\n",
    "        index=False\n",
    "    ) # Save merged data to save time in future runs\n",
    "\n",
    "\n",
    "# Data cleaning and merging\n",
    "\n",
    "data = merged_df[[\"date\", \"ticker\", \"floor\", \"close\"]].sort_values([\"ticker\", \"date\"]).assign(\n",
    "    returns = lambda df : df.groupby(\"ticker\")[\"close\"].pct_change(),\n",
    "    log_returns_pct = lambda df : np.log(df[\"close\"] / df.groupby(\"ticker\")[\"close\"].shift(1))*100\n",
    ")\n",
    "\n",
    "data = data.loc[data[\"ticker\"].str.len()==3] # Eliminate ETF, and indeces\n",
    "\n",
    "data[\"limit\"] = data[\"floor\"].map(limits)\n",
    "outliers = data.loc[data[\"returns\"].abs() > data[\"limit\"]]\n",
    "clean_df = data.drop(outliers.index) # Remove outliers\n",
    "print(f\"% of observations removed: {round((len(outliers)/len(data))*100, 2)}%\")\n",
    "\n",
    "# NOTE: try out different samples of stocks\n",
    "pivoted_df = clean_df.pivot_table(values=\"returns\", index=\"date\", columns=\"ticker\") # Pivot data for better usability\n",
    "pivoted_df = pivoted_df[ticker_list].dropna()\n",
    "\n",
    "display(pivoted_df.describe())\n",
    "train_df, test_df = split_train_test(pivoted_df)\n",
    "\n",
    "# Demean returns\n",
    "train_mean = train_df.mean()\n",
    "dm_train_df = train_df - train_mean\n",
    "dm_test_df = test_df - train_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e69f927",
   "metadata": {},
   "source": [
    "Why demean the returns?\n",
    "\n",
    "Volatility models like BEKK aim to model the covariance structure, not the mean.\n",
    "By removing the mean from the return, it tells the model to focus on modeling volatility clustering and correlations, as well as preventing the mean return from contaminating the volatility dynamics\n",
    "\n",
    "The mean from the training set will be used to demean the test set to simulate real world situation.\n",
    "\n",
    "What's the differences between using static mean and moving average?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b128699c",
   "metadata": {},
   "source": [
    "### ARCH\n",
    "An ARCH model is used to predict volatility at a future time step, with the parameter $q$ as the number of lag squared residual error to include in the model \n",
    "ARCH uses returns or residuals as volatility shocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d012ebda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:      1,   Func. Count:      5,   Neg. LLF: 129357.59275774604\n",
      "Iteration:      2,   Func. Count:     12,   Neg. LLF: 15382.891572734194\n",
      "Iteration:      3,   Func. Count:     19,   Neg. LLF: 13170.123600844268\n",
      "Iteration:      4,   Func. Count:     24,   Neg. LLF: 12577.94370714599\n",
      "Iteration:      5,   Func. Count:     28,   Neg. LLF: 12577.939139618782\n",
      "Iteration:      6,   Func. Count:     32,   Neg. LLF: 12577.938832634314\n",
      "Iteration:      7,   Func. Count:     36,   Neg. LLF: 12577.938830220966\n",
      "Iteration:      8,   Func. Count:     39,   Neg. LLF: 12577.938830221014\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 12577.938830220966\n",
      "            Iterations: 8\n",
      "            Function evaluations: 39\n",
      "            Gradient evaluations: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                      Constant Mean - ARCH Model Results                      \n",
       "==============================================================================\n",
       "Dep. Variable:                    REE   R-squared:                       0.000\n",
       "Mean Model:             Constant Mean   Adj. R-squared:                  0.000\n",
       "Vol Model:                       ARCH   Log-Likelihood:               -12577.9\n",
       "Distribution:                  Normal   AIC:                           25161.9\n",
       "Method:            Maximum Likelihood   BIC:                           25182.0\n",
       "                                        No. Observations:                 5951\n",
       "Date:                Fri, Aug 22 2025   Df Residuals:                     5950\n",
       "Time:                        13:14:54   Df Model:                            1\n",
       "                                Mean Model                                \n",
       "==========================================================================\n",
       "                 coef    std err          t      P>|t|    95.0% Conf. Int.\n",
       "--------------------------------------------------------------------------\n",
       "mu             0.0930  2.486e-02      3.740  1.837e-04 [4.426e-02,  0.142]\n",
       "                            Volatility Model                            \n",
       "========================================================================\n",
       "                 coef    std err          t      P>|t|  95.0% Conf. Int.\n",
       "------------------------------------------------------------------------\n",
       "omega          2.8485      0.104     27.378 4.949e-165 [  2.645,  3.052]\n",
       "alpha[1]       0.3751  2.011e-02     18.655  1.156e-77 [  0.336,  0.414]\n",
       "========================================================================\n",
       "\n",
       "Covariance estimator: robust\n",
       "ARCHModelResult, id: 0x251f7d69010"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from arch import arch_model\n",
    "\n",
    "garch = arch_model(pivoted_df[\"REE\"]*100, vol=\"ARCH\")\n",
    "garch.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0797f",
   "metadata": {},
   "source": [
    "### GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bb1eb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:      1,   Func. Count:      6,   Neg. LLF: 6841595578418.32\n",
      "Iteration:      2,   Func. Count:     15,   Neg. LLF: 6411162190.4786215\n",
      "Iteration:      3,   Func. Count:     23,   Neg. LLF: 14540.272691777205\n",
      "Iteration:      4,   Func. Count:     30,   Neg. LLF: 12675.511071518553\n",
      "Iteration:      5,   Func. Count:     38,   Neg. LLF: 12163.288926570278\n",
      "Iteration:      6,   Func. Count:     44,   Neg. LLF: 12162.193236000765\n",
      "Iteration:      7,   Func. Count:     49,   Neg. LLF: 12162.192674535536\n",
      "Iteration:      8,   Func. Count:     54,   Neg. LLF: 12162.192667947853\n",
      "Iteration:      9,   Func. Count:     59,   Neg. LLF: 12162.192667437183\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 12162.192667437183\n",
      "            Iterations: 9\n",
      "            Function evaluations: 59\n",
      "            Gradient evaluations: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                     Constant Mean - GARCH Model Results                      \n",
       "==============================================================================\n",
       "Dep. Variable:                    REE   R-squared:                       0.000\n",
       "Mean Model:             Constant Mean   Adj. R-squared:                  0.000\n",
       "Vol Model:                      GARCH   Log-Likelihood:               -12162.2\n",
       "Distribution:                  Normal   AIC:                           24332.4\n",
       "Method:            Maximum Likelihood   BIC:                           24359.2\n",
       "                                        No. Observations:                 5951\n",
       "Date:                Fri, Aug 22 2025   Df Residuals:                     5950\n",
       "Time:                        13:14:54   Df Model:                            1\n",
       "                                Mean Model                                \n",
       "==========================================================================\n",
       "                 coef    std err          t      P>|t|    95.0% Conf. Int.\n",
       "--------------------------------------------------------------------------\n",
       "mu             0.0747  2.140e-02      3.493  4.770e-04 [3.281e-02,  0.117]\n",
       "                             Volatility Model                             \n",
       "==========================================================================\n",
       "                 coef    std err          t      P>|t|    95.0% Conf. Int.\n",
       "--------------------------------------------------------------------------\n",
       "omega          0.1288  3.058e-02      4.212  2.528e-05 [6.887e-02,  0.189]\n",
       "alpha[1]       0.1241  1.475e-02      8.409  4.131e-17 [9.515e-02,  0.153]\n",
       "beta[1]        0.8474  1.981e-02     42.784      0.000   [  0.809,  0.886]\n",
       "==========================================================================\n",
       "\n",
       "Covariance estimator: robust\n",
       "ARCHModelResult, id: 0x2518914fe60"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from arch import arch_model\n",
    "\n",
    "garch = arch_model(pivoted_df[\"REE\"]*100, vol=\"GARCH\")\n",
    "garch.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd69348",
   "metadata": {},
   "source": [
    "# BEKK-GARCH\n",
    "\n",
    "$$H_t=C'C+A'\\epsilon_{t-1}\\epsilon_{t-1}'+B'H_{t-1}B$$\n",
    "\n",
    "- $H_t$: n_assets x n_assets conditional covariance matrix\n",
    "- $C$: Lower triangular matrix to ensure positive definteness\n",
    "- $A$: Captures the effect of past shocks (ARCH)\n",
    "- $B$: Captures the persistence (GARCH)\n",
    "- $\\epsilon_{t-1}$: Vector of past residuals (demeaned returns)\n",
    "\n",
    "### Usage\n",
    "- Used for small dimensions of 2-3 assets.\n",
    "- Contagion studies, volatility spillovers.\n",
    "\n",
    "### Advantages\n",
    "- Guarantees positive definite covariance matrices\n",
    "- Captures spillover effects across assets\n",
    "- More flexible in modeling asymmetric dependencies\n",
    "\n",
    "### Disadvantages\n",
    "- Computationally heavy, parameter explosion: for $N$ assets, get ~$N^2$ parameters\n",
    "- Harder to estimate high-dimensional data\n",
    "- May overfit with limited data\n",
    "\n",
    "Since the model suffers from the curse of dimensionality, so I can only sample 3 stocks, at around 1000 days windows. This should be tuned later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39a657cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model\n",
    "if \"bekk_results.pkl\" in os.listdir(\"bekk_params\"):\n",
    "    with open(os.path.join(\"bekk_params\", \"bekk_results.pkl\"), \"rb\") as f:\n",
    "        bekk = pickle.load(f)\n",
    "    with open(os.path.join(\"bekk_params\", \"bekk_A.pkl\"), \"rb\") as f:\n",
    "        A = pickle.load(f)\n",
    "    with open(os.path.join(\"bekk_params\", \"bekk_B.pkl\"), \"rb\") as f:\n",
    "        B = pickle.load(f)\n",
    "    with open(os.path.join(\"bekk_params\", \"bekk_C.pkl\"), \"rb\") as f:\n",
    "        C = pickle.load(f) \n",
    "else:\n",
    "    C, A, B, bekk = BEKK_GARCH_MODEL.fit_bekk(dm_train_df.tail(1000).values) # Fit BEKK-GARCH model, dimensions is greately reduced as this model suffers from curse of dimensionality\n",
    "    # Save covariance matrix and model parameters\n",
    "    with open(os.path.join(\"bekk_params\", \"bekk_results.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(bekk, f)\n",
    "    with open(os.path.join(\"bekk_params\", \"bekk_C.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(C, f)\n",
    "    with open(os.path.join(\"bekk_params\", \"bekk_A.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(A, f)\n",
    "    with open(os.path.join(\"bekk_params\", \"bekk_B.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(B, f)\n",
    "\n",
    "# Get BEKK-GARCH fitted covariance        \n",
    "cov_matrix = BEKK_GARCH_MODEL.bekk_fitted_covariances(bekk.x, returns=dm_train_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3aa4b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEKK-GARCH MODEL\n",
      "      \n",
      "- Sharpe Ratio = 0.10774127440649622\n",
      "- Frobenius norm = 5.166031022084115e-05\n",
      "- Stein loss = 13.938248837048857\n",
      "- Correlation loss = 6.2819259389772135\n",
      "- Portfolio aligned loss = 2.7505339551703894e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "last_return = dm_train_df.iloc[-1].to_numpy()\n",
    "last_cov_matrix = cov_matrix[-1]\n",
    "\n",
    "dates_test = dm_test_df.index\n",
    "\n",
    "bekk_port_returns, bekk_port_vars, bekk_act_covs = [], [], [] # Containers\n",
    "\n",
    "# Loop over test period in 20-days non-overlapping horizons\n",
    "for start in range(0, len(dm_test_df) - horizon + 1, horizon):\n",
    "    train_data = pd.concat([dm_train_df, dm_test_df.iloc[:start]])\n",
    "    \n",
    "    # Forecast x step ahead\n",
    "    cov_list = BEKK_GARCH_MODEL.bekk_forecast(\n",
    "        C, A, B, \n",
    "        train_data.values, \n",
    "        horizon=horizon\n",
    "    )\n",
    "\n",
    "    # Aggregate to 20-days covariance forecast\n",
    "    agg_covariance = sum(cov_list)\n",
    "\n",
    "    # Get MVP weights\n",
    "    mvp_weights, weights_dict = minimum_variance_portfolio(agg_covariance , train_df)\n",
    "\n",
    "    # Realized returns from next 20-days\n",
    "    horizon_return = test_df[start:start+horizon]\n",
    "\n",
    "\n",
    "    # Cummulative return\n",
    "    port_return = np.array(horizon_return) @ mvp_weights\n",
    "    bekk_port_returns.append(port_return.sum())\n",
    "\n",
    "    # Actual covariance\n",
    "    act_covariance =  horizon_return.T @ horizon_return\n",
    "    act_var = mvp_weights.T @ act_covariance @ mvp_weights\n",
    "    bekk_port_vars.append(act_var)\n",
    "    bekk_act_covs.append(act_covariance)\n",
    "\n",
    "    # Adjust for next iteration\n",
    "    last_return = np.array(horizon_return.iloc[[-1]])[0]\n",
    "    last_cov_matrix = np.array(bekk_act_covs[-1])\n",
    "\n",
    "bekk_garch_results =  pd.DataFrame({\n",
    "    \"date\":dates_test[horizon-1::horizon][:len(bekk_port_returns)], # End of each horizon\n",
    "    \"realized_return\":bekk_port_returns,\n",
    "    \"realized_variance\":bekk_port_vars \n",
    "})\n",
    "\n",
    "bekk_sr = bekk_garch_results[\"realized_return\"].mean()/bekk_garch_results[\"realized_return\"].std()\n",
    "bekk_frob = frobenius_loss(H_pred=agg_covariance, H_true=act_covariance)\n",
    "bekk_stein = stein_loss(H_pred=agg_covariance, H_true=act_covariance)\n",
    "bekk_corr_loss = correlation_loss(H_pred=agg_covariance, H_true=act_covariance)\n",
    "bekk_port_aligned = portfolio_aligned_loss(agg_covariance, act_covariance, mvp_weights)\n",
    "\n",
    "print(f\"\"\"\n",
    "BEKK-GARCH MODEL\n",
    "      \n",
    "- Sharpe Ratio = {bekk_sr}\n",
    "- Frobenius norm = {bekk_frob}\n",
    "- Stein loss = {bekk_stein}\n",
    "- Correlation loss = {bekk_corr_loss}\n",
    "- Portfolio aligned loss = {bekk_port_aligned}\n",
    "\"\"\")\n",
    "\n",
    "# Save results\n",
    "with open(os.path.join(\"bekk_params\", \"bekk_portfolio_return.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(bekk_port_returns, f)\n",
    "with open(os.path.join(\"bekk_params\", \"bekk_portfolio_variance.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(bekk_port_vars, f)\n",
    "with open(os.path.join(\"bekk_params\", \"bekk_actual_covariance.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(bekk_act_covs, f)\n",
    "with open(os.path.join(\"bekk_params\", \"bekk_forecast_covariance.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(agg_covariance, f)\n",
    "with open(os.path.join(\"bekk_params\", \"bekk_weights.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f524f0",
   "metadata": {},
   "source": [
    "# DCC-GARCH\n",
    "Is a two-step model:\n",
    "1. Estimate univariate GARCH for each asset's variance\n",
    "\n",
    "$$h_{i,t}=\\omega_i + \\alpha_i \\epsilon_{i,t-1}^2+\\beta_i h_{i,t-1}$$\n",
    "\n",
    "2. Model the correlations dynamically:\n",
    "\n",
    "$$H_t=D_tR_tD_t$$\n",
    "\n",
    "With:\n",
    "- $D_t=diag(\\sqrt{h_{1,t}},...,\\sqrt{h_{N,t}})$: matrix of standard deviations\n",
    "- $R_t$: dynamic correlation matrix, updated via:\n",
    "$$Q_t=(1-\\alpha-\\beta)\\overline{Q}+\\alpha \\epsilon_{t-1} \\epsilon_{t-1}' + \\beta Q_{t-1}$$\n",
    "$$R_t=diag(Q_t)^{-\\frac{1}{2}} Q_t diag(Q_t)^{-\\frac{1}{2}}$$\n",
    "- $\\overline{Q}$: Unconditional correlation matrix\n",
    "\n",
    "### Usage\n",
    "- Designed for high-dimensional portfolios (10-100 assets)\n",
    "- Used for correlation dynamics, portfolio optimization, hedging\n",
    "\n",
    "### Advantages\n",
    "- Computationally efficient, espically in large dimensions\n",
    "- Decouples volatility (diagonal part) and correlation (off-diagonal part)\n",
    "- Easier to interpret dynamic correlation structure\n",
    "- Widely used in epirical finance\n",
    "\n",
    "### Disadvantages\n",
    "- Less flexible than BEKK\n",
    "- Correlations may be biased if univariage GARCH models are misspecified\n",
    "- Does not directly capture cross-variance spillovers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6237cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_train_df = dm_train_df.astype(float)\n",
    "h_mat, eps_mat, garch_params = DCC_GARCH_MODEL.fit_univariate_garch(dm_train_df) # Fit univariate GARCH for each stock\n",
    "dcc = DCC_GARCH_MODEL.fit_dcc(eps_mat) # Fit DCC(1,1) on standardized residuals\n",
    "cov_matrix = DCC_GARCH_MODEL.build_covmatrix(h_mat, dcc[\"Rt\"]) # Get full list of conditinoal covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "322cedb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DCC-GARCH MODEL      \n",
      "\n",
      "- Sharpe Ratio = 0.14936500680575968\n",
      "- Frobenius norm = 8.740778400773257e-05\n",
      "- Stein loss = 10.998319241644367\n",
      "- Correlation loss = 0.13916569628533665\n",
      "- Portfolio aligned loss = 2.67880626484674e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "\n",
    "horizon = 20\n",
    "\n",
    "last_return = dm_train_df.iloc[-1].to_numpy()\n",
    "last_cov_matrix = cov_matrix[-1]\n",
    "\n",
    "dates_test = dm_test_df.index\n",
    "\n",
    "dcc_port_returns, dcc_port_vars, dcc_act_covs = [], [], [] # Containers\n",
    "\n",
    "# Loop over test period in 20-days non-overlapping horizons\n",
    "for start in range(0, len(dm_test_df) - horizon + 1, horizon):\n",
    "    train_data = pd.concat([dm_train_df, dm_test_df.iloc[:start]])\n",
    "    \n",
    "    # Forecast x step ahead\n",
    "    cov_list, _, _, _ = DCC_GARCH_MODEL.forecast_dcc_multi_step(\n",
    "        h_last=h_mat[-1],\n",
    "        r_last=last_return,\n",
    "        garch_params=garch_params,\n",
    "        eps_last=eps_mat[-1],\n",
    "        Q_last=dcc[\"Qt\"][-1],\n",
    "        dcc_params=dcc,\n",
    "        S=dcc[\"S\"]\n",
    "    )\n",
    "\n",
    "    # Aggregate to 20-days covariance forecast\n",
    "    agg_covariance = sum(cov_list)\n",
    "\n",
    "    # Get MVP weights\n",
    "    mvp_weights, weights_dict = minimum_variance_portfolio(agg_covariance , train_df)\n",
    "\n",
    "    # Realized returns from next 20-days\n",
    "    horizon_return = test_df[start:start+horizon]\n",
    "\n",
    "\n",
    "    # Cummulative return\n",
    "    port_return = np.array(horizon_return) @ mvp_weights\n",
    "    dcc_port_returns.append(port_return.sum())\n",
    "\n",
    "    # Actual covariance\n",
    "    act_covariance =  horizon_return.T @ horizon_return\n",
    "    act_var = mvp_weights.T @ act_covariance @ mvp_weights\n",
    "    dcc_port_vars.append(act_var)\n",
    "    dcc_act_covs.append(act_covariance)\n",
    "\n",
    "    # Adjust for next iteration\n",
    "    last_return = np.array(horizon_return.iloc[[-1]])[0]\n",
    "    last_cov_matrix = np.array(dcc_act_covs[-1])\n",
    "\n",
    "dcc_garch_results =  pd.DataFrame({\n",
    "    \"date\":dates_test[horizon-1::horizon][:len(dcc_port_returns)], # End of each horizon\n",
    "    \"realized_return\":dcc_port_returns,\n",
    "    \"realized_variance\":dcc_port_vars \n",
    "})\n",
    "\n",
    "dcc_sr = dcc_garch_results[\"realized_return\"].mean()/dcc_garch_results[\"realized_return\"].std()\n",
    "dcc_frob = frobenius_loss(H_pred=agg_covariance, H_true=act_covariance)\n",
    "dcc_stein = stein_loss(H_pred=agg_covariance, H_true=act_covariance)\n",
    "dcc_corr_loss = correlation_loss(H_pred=agg_covariance, H_true=act_covariance)\n",
    "dcc_port_aligned = portfolio_aligned_loss(agg_covariance, act_covariance, mvp_weights)\n",
    "\n",
    "print(f\"\"\"\n",
    "DCC-GARCH MODEL      \n",
    "\n",
    "- Sharpe Ratio = {dcc_sr}\n",
    "- Frobenius norm = {dcc_frob}\n",
    "- Stein loss = {dcc_stein}\n",
    "- Correlation loss = {dcc_corr_loss}\n",
    "- Portfolio aligned loss = {dcc_port_aligned}\n",
    "\"\"\")\n",
    "\n",
    "# Save results\n",
    "with open(os.path.join(\"dcc_results\", \"dcc_portfolio_return.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(dcc_port_returns, f)\n",
    "with open(os.path.join(\"dcc_results\", \"dcc_portfolio_variance.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(dcc_port_vars, f)\n",
    "with open(os.path.join(\"dcc_results\", \"dcc_actual_covariance.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(dcc_act_covs, f)\n",
    "with open(os.path.join(\"dcc_results\", \"dcc_forecast_covariance.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(agg_covariance, f)\n",
    "with open(os.path.join(\"dcc_results\", \"dcc_weights.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2678771",
   "metadata": {},
   "source": [
    "# LSTM-BEKK\n",
    "\n",
    "$$H_t = C'C + C_t'C_t + a r_{t-1} r_{t-1}' + b H_{t-1}$$\n",
    "\n",
    "$C_t$ is dynamically updated through an LSTM network $\\overline{C_t}=LSTM(h_{t-1}, r_{t-1})$, with $C_t=LowerTriangular(\\overline{C_t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bc93ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM training parameters\n",
    "# Fit model\n",
    "\n",
    "save_path = os.path.join(\"lstm_model\", \"lstm_bekk_model_state_dict.pt\")\n",
    "\n",
    "if \"lstm_bekk_model_state_dict.pt\" in os.listdir(\"lstm_model\"):\n",
    "    # Load model weights from file\n",
    "    lstm_bekk_model = LSTM_BEKK_MODEL.load_model(\n",
    "        path=save_path,\n",
    "        n_assets=dm_train_df.shape[1],\n",
    "        config=LSTM_BEKK_MODEL.LSTM_BEKK_config(\n",
    "            hidden_size=3, # Same as number of assets\n",
    "            num_layers=1,\n",
    "            dropout=0.1,\n",
    "            lr=0.001,\n",
    "            epochs=600\n",
    "        )\n",
    "    )\n",
    "    lstm_bekk_model.load_state_dict(\n",
    "        torch.load(save_path)\n",
    "    )\n",
    "    lstm_bekk_model.eval()\n",
    "else:\n",
    "    \n",
    "    lstm_bekk_model = LSTM_BEKK_MODEL.fit_lstm_bekk(\n",
    "        returns_df=dm_train_df,\n",
    "        hidden_size=3, # Same as number of assets\n",
    "        num_layers=1,\n",
    "        dropout=0.1,\n",
    "        lr=0.001,\n",
    "        epochs=600\n",
    "    )\n",
    "\n",
    "    # Save model weights using torch.save\n",
    "    torch.save(lstm_bekk_model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3290d25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM-BEKK MODEL\n",
      "\n",
      "- Sharpe Ratio = 0.2126058763067286\n",
      "- Frobenius loss = 146.1823761667859 \n",
      "- Correlation loss = 0.9021173408377456\n",
      "- Portfolio aligned loss = 11.8648825858575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "cov_matrix = lstm_bekk_model.covariance(dm_train_df)\n",
    "last_return = dm_train_df.iloc[-1].to_numpy()\n",
    "last_cov_matrix = cov_matrix[-1]\n",
    "\n",
    "### 20 stepts forecast\n",
    "cov_matrix_20_steps = lstm_bekk_model.forecast_multi_step(\n",
    "    last_returns=last_return,\n",
    "    last_cov=last_cov_matrix,\n",
    "    steps=20,\n",
    "    method=\"zero\"\n",
    ")\n",
    "# Evaluate MVP\n",
    "\n",
    "last_return = dm_train_df.iloc[-1].to_numpy()\n",
    "last_cov_matrix = cov_matrix[-1]\n",
    "\n",
    "train_mean = train_df.mean()\n",
    "demean_test_df = test_df - train_mean # Demean with in-sample mean\n",
    "dates_test = demean_test_df.index\n",
    "\n",
    "lstm_port_returns, lstm_port_vars, lstm_act_covs = [], [], [] # Containers\n",
    "\n",
    "# Loop over test period in 20-days non-overlapping horizons\n",
    "for start in range(0, len(demean_test_df) - horizon + 1, horizon):\n",
    "    train_data = pd.concat([train_df, demean_test_df.iloc[:start]])\n",
    "    \n",
    "    # Forecast x step ahead\n",
    "    cov_list = lstm_bekk_model.forecast_multi_step(\n",
    "        last_returns=last_return,\n",
    "        last_cov=last_cov_matrix,\n",
    "        steps=horizon,\n",
    "        method=\"zero\"\n",
    "    )\n",
    "\n",
    "    # Aggregate to 20-days covariance forecast\n",
    "    agg_covariance = sum(cov_list)\n",
    "\n",
    "    # Get MVP weights\n",
    "    mvp_weights, weights_dict = minimum_variance_portfolio(agg_covariance , train_df)\n",
    "\n",
    "    # Realized returns from next 20-days\n",
    "    horizon_return = test_df[start:start+horizon]\n",
    "\n",
    "    # Cummulative return\n",
    "    port_return = np.array(horizon_return) @ mvp_weights\n",
    "    lstm_port_returns.append(port_return.sum())\n",
    "\n",
    "    # Actual covariance\n",
    "    act_covariance =  horizon_return.T @ horizon_return\n",
    "    act_var = mvp_weights.T @ act_covariance @ mvp_weights\n",
    "    lstm_port_vars.append(act_var)\n",
    "    lstm_act_covs.append(act_covariance)\n",
    "\n",
    "    # Adjust for next iteration\n",
    "    last_return = np.array(horizon_return.iloc[[-1]])[0]\n",
    "    last_cov_matrix = np.array(lstm_act_covs[-1])\n",
    "\n",
    "lstm_bekk_results =  pd.DataFrame({\n",
    "    \"date\":dates_test[horizon-1::horizon][:len(lstm_port_returns)], # End of each horizon\n",
    "    \"realized_return\":lstm_port_returns,\n",
    "    \"realized_variance\":lstm_port_vars \n",
    "})\n",
    "\n",
    "lstm_sr = lstm_bekk_results[\"realized_return\"].mean()/lstm_bekk_results[\"realized_return\"].std()\n",
    "lstm_frob = frobenius_loss(H_pred=agg_covariance, H_true=act_covariance)\n",
    "lstm_stein = stein_loss(H_pred=agg_covariance, H_true=act_covariance)\n",
    "lstm_corr_loss = correlation_loss(H_pred=agg_covariance, H_true=act_covariance)\n",
    "lstm_port_aligned = portfolio_aligned_loss(agg_covariance, act_covariance, mvp_weights)\n",
    "\n",
    "print(f\"\"\"\n",
    "LSTM-BEKK MODEL\n",
    "\n",
    "- Sharpe Ratio = {lstm_sr}\n",
    "- Frobenius loss = {lstm_frob} \n",
    "- Correlation loss = {lstm_corr_loss}\n",
    "- Portfolio aligned loss = {lstm_port_aligned}\n",
    "\"\"\")\n",
    "\n",
    "# Save results\n",
    "with open(os.path.join(\"lstm_model\", \"lstm_portfolio_return.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(lstm_port_returns, f)\n",
    "with open(os.path.join(\"lstm_model\", \"lstm_portfolio_variance.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(lstm_port_vars, f)\n",
    "with open(os.path.join(\"lstm_model\", \"lstm_actual_covariance.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(lstm_act_covs, f)\n",
    "with open(os.path.join(\"lstm_model\", \"lstm_forecast_covariance.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(agg_covariance, f)\n",
    "with open(os.path.join(\"lstm_model\", \"lstm_weights.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1f32a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = pd.DataFrame({\n",
    "    \"Ratios\":[\"Sharpe ratio\", \"Frobenius loss\", \"Correlation loss\", \"Portflio aligned loss\", \"Stein loss\"],\n",
    "    \"BEKK-GARCH\": [round(i,4) for i in [bekk_sr, bekk_frob, bekk_corr_loss, bekk_port_aligned, bekk_stein]],\n",
    "    \"DCC-GARCH\":[round(i, 4) for i in [dcc_sr, dcc_frob, dcc_corr_loss, dcc_port_aligned, dcc_stein]],\n",
    "    \"LSTM-BEKK\":[round(i, 4) for i in [lstm_sr, lstm_frob, lstm_corr_loss, lstm_port_aligned, lstm_stein]],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bbf3f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval.to_clipboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_bekk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
