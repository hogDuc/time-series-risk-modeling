{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08f9545",
   "metadata": {},
   "source": [
    "# Supported Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b18d1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import functions\n",
    "from functions import *\n",
    "from evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1747dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of observations removed: 1.05%\n"
     ]
    }
   ],
   "source": [
    "# ticker_list = ['REE', 'SAM', 'HAP', 'GMD', 'GIL', 'TMS', 'SAV', 'DHA', 'MHC', 'HAS'] # 10 stocks with the most observations\n",
    "ticker_list = ['REE', 'SAM', 'HAP'] # 3 stocks with the most observations\n",
    "limits = {\n",
    "    'hose':0.07,\n",
    "    'hnx':0.1,\n",
    "    'upcom':0.15\n",
    "}\n",
    "# Read and merge into 1 dataset\n",
    "\n",
    "if \"stock_data.csv\" in os.listdir(\"data\"):\n",
    "    merged_df = pd.read_csv(\n",
    "        os.path.join(\"data\", \"stock_data.csv\"),\n",
    "        index_col=None\n",
    "    ).assign(\n",
    "        date = lambda df : pd.to_datetime(df[\"date\"])\n",
    "    )\n",
    "else:\n",
    "    # Read and merge data\n",
    "    hnx = pd.read_csv(os.path.join(\"data\", \"CafeF.HNX.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"hnx\"\n",
    "    )\n",
    "    hsx = pd.read_csv(os.path.join(\"data\", \"CafeF.HSX.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"hose\"\n",
    "    )\n",
    "    upcom = pd.read_csv(os.path.join(\"data\", \"CafeF.UPCOM.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"upcom\"\n",
    "    )\n",
    "    indexes = pd.read_csv(os.path.join(\"data\", \"CafeF.INDEX.Upto06.08.2025.csv\")).assign(\n",
    "        floor = \"index\"\n",
    "    )\n",
    "\n",
    "    # Rename columns\n",
    "    hnx, hsx, upcom, indexes = [\n",
    "        df.rename(columns={\n",
    "            \"<Ticker>\":\"ticker\",\n",
    "            \"<DTYYYYMMDD>\":\"date\",\n",
    "            \"<Open>\":\"open\",\n",
    "            \"<High>\":\"high\",\n",
    "            \"<Low>\":\"low\",\n",
    "            \"<Close>\":\"close\",\n",
    "            \"<Volume>\":\"volume\"\n",
    "        }) for df in [hnx, hsx, upcom, indexes]\n",
    "    ]\n",
    "        \n",
    "    # Merge and clean data\n",
    "    # UPCOM has missing tickers for some reason\n",
    "    merged_df = pd.concat(\n",
    "        [hnx, hsx, upcom, indexes],\n",
    "        axis=0\n",
    "    ).reset_index(drop=True).dropna(subset=\"ticker\")\\\n",
    "    .assign(\n",
    "        date=lambda df : df[\"date\"].astype(str).apply(lambda x: datetime.strptime(x, \"%Y%m%d\").date())\n",
    "    )\n",
    "    merged_df.to_csv(\n",
    "        os.path.join(\"data\", \"stock_data.csv\"),\n",
    "        index=False\n",
    "    ) # Save merged data to save time in future runs\n",
    "\n",
    "\n",
    "data = merged_df.sort_values([\"ticker\", \"date\"]).assign(\n",
    "    returns = lambda df : df.groupby(\"ticker\")[\"close\"].pct_change()\n",
    ")\n",
    "\n",
    "data = data.loc[data[\"ticker\"].str.len()==3] # Eliminate ETF, and indeces\n",
    "\n",
    "data[\"limit\"] = data[\"floor\"].map(limits)\n",
    "outliers = data.loc[data[\"returns\"].abs() > data[\"limit\"]]\n",
    "clean_df = data.drop(outliers.index) # Remove outliers\n",
    "print(f\"% of observations removed: {round((len(outliers)/len(data))*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed23ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_data = data.pivot_table(\n",
    "    columns=\"ticker\", \n",
    "    values=[\"open\", \"high\", \"low\", \"close\"], \n",
    "    index=\"date\"\n",
    ")\n",
    "pivoted_data.columns = pivoted_data.columns.swaplevel(0, 1)\n",
    "pivoted_data = pivoted_data.sort_index(axis=1, level=0)\n",
    "pivoted_data = pivoted_data.loc[:, pivoted_data.columns.get_level_values(0).isin(ticker_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225bb464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parkinson_variance(high, low):\n",
    "    \"\"\"\n",
    "    Parkinson variance estimator with high and low prices\n",
    "    var_{Pt} = [ln(H_t/L_t)^2]/(4*ln2)\n",
    "    \"\"\"\n",
    "    return (np.log(high/low)**2) / (4*np.log(2))\n",
    "\n",
    "def range_based_covariance_matrix(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute range-based covariance matrix for multiple assets\n",
    "    Args:\n",
    "        data: TxM dataframe in MultiIndex format (asset, price_type). Example: ('HPG', 'high), ('HPG', 'low'),...\n",
    "    Returns:\n",
    "        Range-based covariance matrix for multiple assets\n",
    "    \"\"\"\n",
    "\n",
    "    tickers = data.columns.get_level_values(0).unique(0)\n",
    "    n_assets = len(tickers)\n",
    "    cov_matrices = {}\n",
    "\n",
    "    for date, row in data.iterrows():\n",
    "        variances = {}\n",
    "        for asset in tickers:\n",
    "            high_price, low_price = row[asset, 'high'], row[asset, 'low']\n",
    "            variances[asset] = parkinson_variance(high_price, low_price)\n",
    "\n",
    "        cov_matrix = pd.DataFrame(\n",
    "            np.zeros((n_assets, n_assets)),\n",
    "            index=tickers,\n",
    "            columns=tickers\n",
    "        ) # Initialize covariance matrix\n",
    "\n",
    "        # Fill diagonal with variances estimated with Parkinson\n",
    "        for asset in tickers:\n",
    "            cov_matrix.loc[asset, asset] = variances[asset]\n",
    "\n",
    "        # Off-diagonals\n",
    "        for i, asset_i in enumerate(tickers):\n",
    "            for j, asset_j in enumerate(tickers):\n",
    "                if j>i: # Only get the upper triangular\n",
    "                    high_sum = row[asset_i, \"high\"] + row[(asset_j, 'high')]\n",
    "                    low_sum = row[asset_i, \"low\"] + row[asset_j, 'low']\n",
    "                    var_sum = parkinson_variance(high_sum, low_sum)\n",
    "\n",
    "                    cov = 0.5 * (var_sum - variances[asset_i] - variances[asset_j])\n",
    "                    cov_matrix.loc[asset_i, asset_j] = cov\n",
    "                    cov_matrix.loc[asset_j, asset_i] = cov\n",
    "                \n",
    "        cov_matrices[date] = cov_matrix\n",
    "    \n",
    "    return cov_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b247f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrices = range_based_covariance_matrix(pivoted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec586049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_bekk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
