{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08f9545",
   "metadata": {},
   "source": [
    "# Supported Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b18d1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import functions\n",
    "from functions import *\n",
    "from evaluate import *\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "1747dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of observations removed: 1.05%\n"
     ]
    }
   ],
   "source": [
    "# ticker_list = ['REE', 'SAM', 'HAP', 'GMD', 'GIL', 'TMS', 'SAV', 'DHA', 'MHC', 'HAS'] # 10 stocks with the most observations\n",
    "ticker_list = ['REE', 'SAM', 'HAP'] # 3 stocks with the most observations\n",
    "limits = {\n",
    "    'hose':0.07,\n",
    "    'hnx':0.1,\n",
    "    'upcom':0.15\n",
    "}\n",
    "# Read and merge into 1 dataset\n",
    "\n",
    "if \"stock_data.csv\" in os.listdir(\"data\"):\n",
    "    merged_df = pd.read_csv(\n",
    "        os.path.join(\"data\", \"stock_data.csv\"),\n",
    "        index_col=None\n",
    "    ).assign(\n",
    "        date = lambda df : pd.to_datetime(df[\"date\"])\n",
    "    )\n",
    "else:\n",
    "    # Read and merge data\n",
    "    hnx = pd.read_csv(os.path.join(\"data\", \"CafeF.HNX.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"hnx\"\n",
    "    )\n",
    "    hsx = pd.read_csv(os.path.join(\"data\", \"CafeF.HSX.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"hose\"\n",
    "    )\n",
    "    upcom = pd.read_csv(os.path.join(\"data\", \"CafeF.UPCOM.Upto31.07.2025.csv\")).assign(\n",
    "        floor = \"upcom\"\n",
    "    )\n",
    "    indexes = pd.read_csv(os.path.join(\"data\", \"CafeF.INDEX.Upto06.08.2025.csv\")).assign(\n",
    "        floor = \"index\"\n",
    "    )\n",
    "\n",
    "    # Rename columns\n",
    "    hnx, hsx, upcom, indexes = [\n",
    "        df.rename(columns={\n",
    "            \"<Ticker>\":\"ticker\",\n",
    "            \"<DTYYYYMMDD>\":\"date\",\n",
    "            \"<Open>\":\"open\",\n",
    "            \"<High>\":\"high\",\n",
    "            \"<Low>\":\"low\",\n",
    "            \"<Close>\":\"close\",\n",
    "            \"<Volume>\":\"volume\"\n",
    "        }) for df in [hnx, hsx, upcom, indexes]\n",
    "    ]\n",
    "        \n",
    "    # Merge and clean data\n",
    "    # UPCOM has missing tickers for some reason\n",
    "    merged_df = pd.concat(\n",
    "        [hnx, hsx, upcom, indexes],\n",
    "        axis=0\n",
    "    ).reset_index(drop=True).dropna(subset=\"ticker\")\\\n",
    "    .assign(\n",
    "        date=lambda df : df[\"date\"].astype(str).apply(lambda x: datetime.strptime(x, \"%Y%m%d\").date())\n",
    "    )\n",
    "    merged_df.to_csv(\n",
    "        os.path.join(\"data\", \"stock_data.csv\"),\n",
    "        index=False\n",
    "    ) # Save merged data to save time in future runs\n",
    "\n",
    "\n",
    "data = merged_df.sort_values([\"ticker\", \"date\"]).assign(\n",
    "    returns = lambda df : df.groupby(\"ticker\")[\"close\"].pct_change()\n",
    ")\n",
    "\n",
    "data = data.loc[data[\"ticker\"].str.len()==3] # Eliminate ETF, and indeces\n",
    "\n",
    "data[\"limit\"] = data[\"floor\"].map(limits)\n",
    "outliers = data.loc[data[\"returns\"].abs() > data[\"limit\"]]\n",
    "clean_df = data.drop(outliers.index) # Remove outliers\n",
    "print(f\"% of observations removed: {round((len(outliers)/len(data))*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "225bb464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parkinson_variance(high, low):\n",
    "    \"\"\"\n",
    "    Parkinson variance estimator with high and low prices\n",
    "    var_{Pt} = [ln(H_t/L_t)^2]/(4*ln2)\n",
    "    \"\"\"\n",
    "    return (np.log(high/low)**2) / (4*np.log(2))\n",
    "\n",
    "def range_based_covariance_matrix(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute range-based covariance matrix for multiple assets\n",
    "    Args:\n",
    "        data: TxM dataframe in MultiIndex format (asset, price_type). Example: ('HPG', 'high), ('HPG', 'low'),...\n",
    "    Returns:\n",
    "        Range-based covariance matrix for multiple assets\n",
    "    \"\"\"\n",
    "\n",
    "    tickers = data.columns.get_level_values(0).unique(0)\n",
    "    n_assets = len(tickers)\n",
    "    cov_matrices = {}\n",
    "\n",
    "    for date, row in data.iterrows():\n",
    "        variances = {}\n",
    "        for asset in tickers:\n",
    "            high_price, low_price = row[asset, 'high'], row[asset, 'low']\n",
    "            variances[asset] = parkinson_variance(high_price, low_price)\n",
    "\n",
    "        cov_matrix = pd.DataFrame(\n",
    "            np.zeros((n_assets, n_assets)),\n",
    "            index=tickers,\n",
    "            columns=tickers\n",
    "        ) # Initialize covariance matrix\n",
    "\n",
    "        # Fill diagonal with variances estimated with Parkinson\n",
    "        for asset in tickers:\n",
    "            cov_matrix.loc[asset, asset] = variances[asset]\n",
    "\n",
    "        # Off-diagonals\n",
    "        for i, asset_i in enumerate(tickers):\n",
    "            for j, asset_j in enumerate(tickers):\n",
    "                if j>i: # Only get the upper triangular\n",
    "                    high_sum = row[asset_i, \"high\"] + row[(asset_j, 'high')]\n",
    "                    low_sum = row[asset_i, \"low\"] + row[asset_j, 'low']\n",
    "                    var_sum = parkinson_variance(high_sum, low_sum)\n",
    "\n",
    "                    cov = 0.5 * (var_sum - variances[asset_i] - variances[asset_j])\n",
    "                    cov_matrix.loc[asset_i, asset_j] = cov\n",
    "                    cov_matrix.loc[asset_j, asset_i] = cov\n",
    "                \n",
    "        cov_matrices[date] = cov_matrix\n",
    "    \n",
    "    return cov_matrices\n",
    "\n",
    "def cholesky_decomposition(\n",
    "    G: np.ndarray,\n",
    "    tol=1e-12,\n",
    "    jitter_start=1e-12,\n",
    "    jitter_max=1e-3\n",
    "):\n",
    "    # Symmetrize\n",
    "    Gs = 0.5 * (G + G.T)\n",
    "\n",
    "    try:\n",
    "        upper_triang = np.linalg.cholesky(Gs, upper=True)\n",
    "        # return upper_triang\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Eigenvalue correction\n",
    "        w, Q = np.linalg.eigh(Gs)\n",
    "        w_clipped = np.maximum(w, tol)\n",
    "        G_corr = (Q * w_clipped) @ Q.T\n",
    "\n",
    "        try:\n",
    "            upper_triang = np.linalg.cholesky(G_corr, upper=True)\n",
    "            # return upper_triang\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Final fallback: escalating diagonal jitter\n",
    "            jitter = jitter_start\n",
    "            I = np.eye(G.shape[0])\n",
    "            while jitter <= jitter_max:\n",
    "                try:\n",
    "                    upper_triang = np.linalg.cholesky(G_corr + jitter * I, upper=True)\n",
    "                    # return upper_triang\n",
    "                except np.linalg.LinAlgError:\n",
    "                    jitter *= 10.0\n",
    "            raise np.linalg.LinAlgError(\"Cholesky failed: matrix far from positive definite even after eigenvalue clipping and jitter\")\n",
    "    \n",
    "    return upper_triang\n",
    "# Step 3: For each entry of the cholesky factor, construct and train the autoregressive SVR model\n",
    "\n",
    "def get_cholesky_series(chol_factors):\n",
    "    dates = list(chol_factors.keys())\n",
    "    P0 = chol_factors[dates[0]] # The first upper triangular matrix\n",
    "    assets = P0.columns\n",
    "    series_dict = {}\n",
    "    n_assets = len(assets)\n",
    "\n",
    "    for i in range(n_assets):\n",
    "        for j in range(i, n_assets):\n",
    "            series_dict[(i, j)] = pd.Series(\n",
    "                [chol_factors[d].iloc[i, j] for d in dates],\n",
    "                index=dates\n",
    "            )\n",
    "    \n",
    "    return series_dict\n",
    "\n",
    "def fit_SVR(series, lags=15):\n",
    "    \"\"\"\n",
    "    Fit SVR to Cholesky entry\n",
    "    Returns:\n",
    "        Fitted series\n",
    "    \"\"\"\n",
    "    y = series.values\n",
    "    X = np.column_stack([np.roll(y, k) for k in range(1, lags + 1)])\n",
    "    X, y = X[lags:], y[lags:]\n",
    "    model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SVR(kernel='rbf', C=1.0, epsilon=0.01)\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "def forecast_svr(model, hist, steps=1, lags=15):\n",
    "    \"\"\"\n",
    "    Forecast with SVR\n",
    "    Args:\n",
    "        model: Fitted SVR model\n",
    "        hist: Historical data\n",
    "        steps: Forecast steps\n",
    "        lags: Days to input into training\n",
    "    Returns Cholesky entries\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = []\n",
    "    h = hist.copy()\n",
    "    for _ in range(steps):\n",
    "        x = h[-lags:].reshape(1, -1)\n",
    "        pred = model.predict(x)[0]\n",
    "        preds.append(pred)\n",
    "        h = np.append(h, pred)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def forecast_covariance(chol_factors, horizon=20, lags=20):\n",
    "    series_dict = get_cholesky_series(chol_factors)\n",
    "    models = {\n",
    "        k: fit_SVR(v, lags=lags) for k, v in series_dict.items()\n",
    "    }\n",
    "    forecasts = {\n",
    "        k: forecast_svr(models[k], series_dict[k].values\n",
    "    , steps=horizon, lags=lags) for k in series_dict.keys()\n",
    "    }\n",
    "    n_assets = len(chol_factors[next(iter(chol_factors))]) # number of assets\n",
    "    pred_covs = []\n",
    "    for step in range(horizon):\n",
    "        # Build forecasted P_t\n",
    "        P_fc = np.zeros((n_assets, n_assets)) # Initialize matrix\n",
    "        for (i, j), vals in forecasts.items():\n",
    "            P_fc[i, j] = vals[step]\n",
    "        \n",
    "        # Covariance forecast\n",
    "        G_fc = P_fc.T @ P_fc\n",
    "        pred_covs.append(G_fc)\n",
    "    \n",
    "    return pred_covs\n",
    "\n",
    "def svr_model_forecast(\n",
    "        train_data, \n",
    "        horizon=20, \n",
    "        lags=30\n",
    "    ):\n",
    "\n",
    "    cov_matrices = range_based_covariance_matrix(train_data)\n",
    "\n",
    "    # Step 2: The matrices are decomposed using Cholesky decomposition in the form G_t = P_t' P_t\n",
    "    # This is to ensure the covariance matrix is always positive definite\n",
    "    chol_factors = {}\n",
    "    for date, cov in cov_matrices.items():\n",
    "        upper_triang = cholesky_decomposition(cov.values)\n",
    "        chol_factors[date] = pd.DataFrame(\n",
    "            upper_triang,\n",
    "            index=cov.index,\n",
    "            columns=cov.columns\n",
    "        )\n",
    "\n",
    "    # Step 3: Predict covariances\n",
    "    pred_covs = forecast_covariance(\n",
    "        chol_factors=chol_factors,\n",
    "        horizon=horizon,\n",
    "        lags=lags # Experiment to find the best lags\n",
    "    )\n",
    "\n",
    "    return pred_covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "481b3938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pivoted_data = clean_df.pivot_table(\n",
    "    columns=\"ticker\", \n",
    "    values=[\"open\", \"high\", \"low\", \"close\", \"returns\"], \n",
    "    index=\"date\"\n",
    ")\n",
    "pivoted_data.columns = pivoted_data.columns.swaplevel(0, 1)\n",
    "pivoted_data = pivoted_data.sort_index(axis=1, level=0)\n",
    "pivoted_data = pivoted_data.loc[:, pivoted_data.columns.get_level_values(0).isin(ticker_list)]\n",
    "pivoted_data = pivoted_data.dropna() # Drop NA\n",
    "train_df, test_df = split_train_test(pivoted_data)\n",
    "\n",
    "test_df_returns = test_df.loc[:, test_df.columns.get_level_values(1) == \"returns\"]\n",
    "test_df_hl = test_df.loc[:, test_df.columns.get_level_values(1).isin([\"high\", 'low'])]\n",
    "train_df_hl = train_df.loc[:, train_df.columns.get_level_values(1).isin([\"high\", 'low'])]\n",
    "train_df_returns = train_df.loc[:, test_df.columns.get_level_values(1) == \"returns\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate NxN range-based covariance matrices of returns for the whole time series. The range-based variances of the returns are the diagonal entries of these matrices.\n",
    "# To estimate the range-based covariance matrices, use estimator of the covariance of the returns, and parkinson estimator of the variance.\n",
    "cov_matrices = range_based_covariance_matrix(train_df)\n",
    "# Step 2: The matrices are decomposed using Cholesky decomposition in the form G_t = P_t' P_t\n",
    "# This is to ensure the covariance matrix is always positive definite\n",
    "\n",
    "chol_factors = {}\n",
    "for date, cov in cov_matrices.items():\n",
    "    upper_triang = cholesky_decomposition(cov.values)\n",
    "    chol_factors[date] = pd.DataFrame(\n",
    "        upper_triang,\n",
    "        index=cov.index,\n",
    "        columns=cov.columns\n",
    "    )\n",
    "\n",
    "# PARAMETERS ====================\n",
    "horizon=20\n",
    "lags = 30\n",
    "#================================\n",
    "\n",
    "act_covs = []\n",
    "dates = []\n",
    "\n",
    "# Step 3: Predict covariances\n",
    "pred_covs = forecast_covariance(\n",
    "    chol_factors=chol_factors,\n",
    "    horizon=horizon,\n",
    "    lags=lags # Experiment to find the best lags\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be5e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SVRCovarianceForecaster:\n",
    "    def __init__(self, tol=1e-12, jitter_start=1e-12, jitter_max=1e-3):\n",
    "        self.tol = tol\n",
    "        self.jitter_start = jitter_start\n",
    "        self.jitter_max = jitter_max\n",
    "\n",
    "    @staticmethod\n",
    "    def parkinson_variance(high, low):\n",
    "        \"\"\"\n",
    "        Parkinson variance estimator with high and low prices\n",
    "        var_{Pt} = [ln(H_t/L_t)^2]/(4*ln2)\n",
    "        \"\"\"\n",
    "        return (np.log(high/low)**2) / (4*np.log(2))\n",
    "\n",
    "    def range_based_covariance_matrix(self, data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Compute range-based covariance matrix for multiple assets\n",
    "        Args:\n",
    "            data: TxM dataframe in MultiIndex format (asset, price_type).\n",
    "        Returns:\n",
    "            Range-based covariance matrix for multiple assets\n",
    "        \"\"\"\n",
    "        tickers = data.columns.get_level_values(0).unique(0)\n",
    "        n_assets = len(tickers)\n",
    "        cov_matrices = {}\n",
    "\n",
    "        for date, row in data.iterrows():\n",
    "            variances = {}\n",
    "            for asset in tickers:\n",
    "                high_price, low_price = row[asset, 'high'], row[asset, 'low']\n",
    "                variances[asset] = self.parkinson_variance(high_price, low_price)\n",
    "\n",
    "            cov_matrix = pd.DataFrame(\n",
    "                np.zeros((n_assets, n_assets)),\n",
    "                index=tickers,\n",
    "                columns=tickers\n",
    "            )\n",
    "\n",
    "            for asset in tickers:\n",
    "                cov_matrix.loc[asset, asset] = variances[asset]\n",
    "\n",
    "            for i, asset_i in enumerate(tickers):\n",
    "                for j, asset_j in enumerate(tickers):\n",
    "                    if j > i:\n",
    "                        high_sum = row[asset_i, \"high\"] + row[(asset_j, 'high')]\n",
    "                        low_sum = row[asset_i, \"low\"] + row[asset_j, 'low']\n",
    "                        var_sum = self.parkinson_variance(high_sum, low_sum)\n",
    "                        cov = 0.5 * (var_sum - variances[asset_i] - variances[asset_j])\n",
    "                        cov_matrix.loc[asset_i, asset_j] = cov\n",
    "                        cov_matrix.loc[asset_j, asset_i] = cov\n",
    "\n",
    "            cov_matrices[date] = cov_matrix\n",
    "\n",
    "        return cov_matrices\n",
    "\n",
    "    def cholesky_decomposition(self, G: np.ndarray):\n",
    "        # Symmetrize\n",
    "        Gs = 0.5 * (G + G.T)\n",
    "        try:\n",
    "            upper_triang = np.linalg.cholesky(Gs, upper=True)\n",
    "        except np.linalg.LinAlgError:\n",
    "            w, Q = np.linalg.eigh(Gs)\n",
    "            w_clipped = np.maximum(w, self.tol)\n",
    "            G_corr = (Q * w_clipped) @ Q.T\n",
    "            try:\n",
    "                upper_triang = np.linalg.cholesky(G_corr, upper=True)\n",
    "            except np.linalg.LinAlgError:\n",
    "                jitter = self.jitter_start\n",
    "                I = np.eye(G.shape[0])\n",
    "                while jitter <= self.jitter_max:\n",
    "                    try:\n",
    "                        upper_triang = np.linalg.cholesky(G_corr + jitter * I, upper=True)\n",
    "                        break\n",
    "                    except np.linalg.LinAlgError:\n",
    "                        jitter *= 10.0\n",
    "                else:\n",
    "                    raise np.linalg.LinAlgError(\"Cholesky failed: matrix far from positive definite even after eigenvalue clipping and jitter\")\n",
    "        return upper_triang\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cholesky_series(chol_factors):\n",
    "        dates = list(chol_factors.keys())\n",
    "        P0 = chol_factors[dates[0]]\n",
    "        assets = P0.columns\n",
    "        series_dict = {}\n",
    "        n_assets = len(assets)\n",
    "        for i in range(n_assets):\n",
    "            for j in range(i, n_assets):\n",
    "                series_dict[(i, j)] = pd.Series(\n",
    "                    [chol_factors[d].iloc[i, j] for d in dates],\n",
    "                    index=dates\n",
    "                )\n",
    "        return series_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_SVR(series, lags=15):\n",
    "        y = series.values\n",
    "        X = np.column_stack([np.roll(y, k) for k in range(1, lags + 1)])\n",
    "        X, y = X[lags:], y[lags:]\n",
    "        model = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SVR(kernel='rbf', C=1.0, epsilon=0.01)\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def forecast_svr(model, hist, steps=1, lags=15):\n",
    "        preds = []\n",
    "        h = hist.copy()\n",
    "        for _ in range(steps):\n",
    "            x = h[-lags:].reshape(1, -1)\n",
    "            pred = model.predict(x)[0]\n",
    "            preds.append(pred)\n",
    "            h = np.append(h, pred)\n",
    "        return preds\n",
    "\n",
    "    def forecast_covariance(self, chol_factors, horizon=20, lags=20):\n",
    "        series_dict = self.get_cholesky_series(chol_factors)\n",
    "        models = {\n",
    "            k: self.fit_SVR(v, lags=lags) for k, v in series_dict.items()\n",
    "        }\n",
    "        forecasts = {\n",
    "            k: self.forecast_svr(models[k], series_dict[k].values, steps=horizon, lags=lags)\n",
    "            for k in series_dict.keys()\n",
    "        }\n",
    "        n_assets = len(chol_factors[next(iter(chol_factors))])\n",
    "        pred_covs = []\n",
    "        for step in range(horizon):\n",
    "            P_fc = np.zeros((n_assets, n_assets))\n",
    "            for (i, j), vals in forecasts.items():\n",
    "                P_fc[i, j] = vals[step]\n",
    "            G_fc = P_fc.T @ P_fc\n",
    "            pred_covs.append(G_fc)\n",
    "        return pred_covs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae38b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get actual covariance\n",
    "act_covs = []\n",
    "pred_covs = []\n",
    "dates = []\n",
    "\n",
    "for i in tqdm(range(len(test_df_returns) - horizon + 1)):\n",
    "    window = test_df_returns.iloc[i : (i+horizon)]\n",
    "    act_cov_matrix = np.cov(test_df_returns.T, bias=True)\n",
    "    act_covs.append(act_cov_matrix)\n",
    "    dates.append(window.index[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a6bdf233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [08:39<00:00,  8.81s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df_returns.columns = train_df_returns.columns.get_level_values(0)\n",
    "svr_pred_covs, svr_port_returns, svr_port_vars, svr_act_covs = [], [], [], []\n",
    "\n",
    "\n",
    "for start in tqdm(range(0, len(test_df_hl) - horizon + 1, horizon)):\n",
    "    train_data = pd.concat([\n",
    "        train_df_hl, test_df_hl.iloc[:start]\n",
    "    ])\n",
    "    cov_list = svr_model_forecast(\n",
    "        train_data=train_data,\n",
    "        horizon=horizon,\n",
    "        lags=lags\n",
    "    )\n",
    "    agg_covariance = sum(cov_list)\n",
    "\n",
    "    # Get MVP weights\n",
    "    mvp_weights, weights_dict = minimum_variance_portfolio(\n",
    "        agg_covariance, train_df_returns\n",
    "    )\n",
    "\n",
    "    horizon_return = test_df_returns[start:start+horizon]\n",
    "\n",
    "    port_return = np.array(horizon_return) @ mvp_weights\n",
    "    svr_port_returns.append(port_return.sum())\n",
    "    \n",
    "    # Actual covariance\n",
    "    act_covariance = np.cov(horizon_return.T)\n",
    "    act_var = mvp_weights.T @ act_covariance @ mvp_weights\n",
    "    \n",
    "    svr_port_vars.append(act_var)\n",
    "    svr_act_covs.append(act_covariance)\n",
    "    svr_pred_covs.append(agg_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e716a036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVR MODEL\n",
      "\n",
      "- Sharpe Ratio = 0.0029304699431166635\n",
      "- Frobenius loss = 0.0006345773102282686 \n",
      "- Correlation loss = 3.6804051333419423\n",
      "- Portfolio aligned loss = 1.254350940183958e-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svr_act_covs = np.array(svr_act_covs)\n",
    "svr_pred_covs = np.array(svr_pred_covs)\n",
    "\n",
    "svr_results = pd.DataFrame({\n",
    "    \"realized_return\":svr_port_returns,\n",
    "    \"realized_variance\":svr_port_vars\n",
    "})\n",
    "\n",
    "svr_sr = svr_results[\"realized_variance\"].mean()/svr_results[\"realized_return\"].std()\n",
    "\n",
    "svr_frob = np.mean([\n",
    "    frobenius_loss(H_pred, H_true) for H_pred, H_true in zip(svr_pred_covs, svr_act_covs)\n",
    "])\n",
    "svr_stein = np.mean([\n",
    "    stein_loss(H_pred, H_true) for H_pred, H_true in zip(svr_pred_covs, svr_act_covs)\n",
    "])\n",
    "svr_corr_loss = np.mean([\n",
    "    correlation_loss(H_pred, H_true) for H_pred, H_true in zip(svr_pred_covs, svr_act_covs)\n",
    "])\n",
    "svr_port_aligned = np.mean([\n",
    "    portfolio_aligned_loss(H_pred, H_true, mvp_weights) for H_pred, H_true in zip(svr_pred_covs, svr_act_covs)\n",
    "])\n",
    "\n",
    "print(f\"\"\"\n",
    "SVR MODEL\n",
    "\n",
    "- Sharpe Ratio = {svr_sr}\n",
    "- Frobenius loss = {svr_frob} \n",
    "- Correlation loss = {svr_corr_loss}\n",
    "- Portfolio aligned loss = {svr_port_aligned}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_bekk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
